{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb57e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ralfh\\GitHub-Desktop\\WiQAS-DataDump\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import fitz\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import requests\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"knowledge_base\"\n",
    "output_dir = \"data\"\n",
    "result_dir = \"results\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"[INFO] Created directory: {output_dir}\")\n",
    "\n",
    "# Create result directory if it doesn't exist\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "    print(f\"[INFO] Created directory: {result_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping A-History-Of-The-Philippines.pdf – already converted.\n",
      "[INFO] Skipping Arkitekturang-Filipino.pdf – already converted.\n",
      "[INFO] Skipping Culture-And-Customs-Of-The-Philippines.pdf – already converted.\n",
      "[INFO] Skipping Filipino-Politics.pdf – already converted.\n",
      "[INFO] Skipping Food-Of-The-Philippines.pdf – already converted.\n",
      "[INFO] Skipping History-Of-The-Flipino-People.pdf – already converted.\n",
      "[INFO] Skipping Philippine-History-Source-Book.pdf – already converted.\n",
      "[INFO] Skipping Philippine-Myths-Legends-And-Folktales.pdf – already converted.\n",
      "[INFO] Skipping Sarap-Essays-On-Philippine-Food.pdf – already converted.\n"
     ]
    }
   ],
   "source": [
    "#Text extraction from PDF files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(input_dir, filename)\n",
    "        json_filename = os.path.splitext(filename)[0] + \".json\"\n",
    "        json_path = os.path.join(output_dir, json_filename)\n",
    "\n",
    "        # Skip if already converted\n",
    "        if os.path.exists(json_path):\n",
    "            print(f\"[INFO] Skipping {filename} – already converted.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Extracting: {filename}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        data = {}\n",
    "\n",
    "        for page_number in tqdm(range(len(doc)), desc=f\"Processing {filename}\"):\n",
    "            page = doc[page_number]\n",
    "            text = page.get_text().strip()\n",
    "            if text:\n",
    "                data[f\"page_{page_number + 1}\"] = {\n",
    "                    \"page\": page_number + 1,\n",
    "                    \"content\": text\n",
    "                }\n",
    "\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Saved to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def clean_text(text):\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix ellipses or multiple punctuation\n",
    "    text = re.sub(r'\\.{3,}', '.', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    # Remove stray characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae214a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 7361.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: A-History-Of-The-Philippines.json\n",
      "[INFO] Preprocessing: Arkitekturang-Filipino.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Arkitekturang-Filipino.json: 100%|██████████| 623/623 [00:00<00:00, 8500.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Arkitekturang-Filipino.json\n",
      "[INFO] Preprocessing: Culture-And-Customs-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Culture-And-Customs-Of-The-Philippines.json: 100%|██████████| 273/273 [00:00<00:00, 6174.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Culture-And-Customs-Of-The-Philippines.json\n",
      "[INFO] Preprocessing: Filipino-Politics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Filipino-Politics.json: 100%|██████████| 379/379 [00:00<00:00, 6642.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Filipino-Politics.json\n",
      "[INFO] Preprocessing: Food-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Food-Of-The-Philippines.json: 100%|██████████| 89/89 [00:00<00:00, 9440.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Food-Of-The-Philippines.json\n",
      "[INFO] Preprocessing: History-Of-The-Flipino-People.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning History-Of-The-Flipino-People.json: 100%|██████████| 645/645 [00:00<00:00, 5679.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: History-Of-The-Flipino-People.json\n",
      "[INFO] Preprocessing: Philippine-History-Source-Book.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Philippine-History-Source-Book.json: 100%|██████████| 643/643 [00:00<00:00, 6917.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Philippine-History-Source-Book.json\n",
      "[INFO] Preprocessing: Philippine-Myths-Legends-And-Folktales.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Philippine-Myths-Legends-And-Folktales.json: 100%|██████████| 148/148 [00:00<00:00, 13176.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Philippine-Myths-Legends-And-Folktales.json\n",
      "[INFO] Preprocessing: Sarap-Essays-On-Philippine-Food.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Sarap-Essays-On-Philippine-Food.json: 100%|██████████| 234/234 [00:00<00:00, 7510.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Sarap-Essays-On-Philippine-Food.json\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "# Process all JSON files\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.lower().endswith(\".json\"):\n",
    "        json_path = os.path.join(data_dir, filename)\n",
    "        print(f\"[INFO] Preprocessing: {filename}\")\n",
    "        \n",
    "        # Load existing data\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Clean text for each page\n",
    "        for key in tqdm(data, desc=f\"Cleaning {filename}\"):\n",
    "            if \"content\" in data[key]:\n",
    "                data[key][\"content\"] = clean_text(data[key][\"content\"])\n",
    "        \n",
    "        # Overwrite file\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"[INFO] Finished cleaning: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3f7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "model_name = \"gemma3:latest\"\n",
    "ollama_url = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e802830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_gemma(text, model=model_name):\n",
    "    prompt = (\n",
    "        f\"Given the following page content:\\n\\n\\\"\\\"\\\"\\n{text.strip()[:1500]}\\n\\\"\\\"\\\"\\n\\n\"\n",
    "        \"Categorize this page broadly, and generate 2 to 5 relevant tags. \"\n",
    "        \"Return only this format:\\n\"\n",
    "        \"Category: <category>\\nTags: <comma-separated tags>\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(ollama_url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            print(f\"[ERROR] Ollama responded with status {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ollama request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b20f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a single page\n",
    "def classify_and_update(page_id, entry):\n",
    "    text = entry.get(\"content\", \"\").strip()\n",
    "    if not text or \"category\" in entry:\n",
    "        return page_id, None\n",
    "    \n",
    "    response = classify_with_gemma(text)\n",
    "    if response:\n",
    "        lines = response.strip().split(\"\\n\")\n",
    "        category, tags = \"unknown\", []\n",
    "        for line in lines:\n",
    "            if line.lower().startswith(\"category:\"):\n",
    "                category = line.split(\":\", 1)[1].strip()\n",
    "            elif line.lower().startswith(\"tags:\"):\n",
    "                tags = [tag.strip() for tag in line.split(\":\", 1)[1].split(\",\")]\n",
    "        \n",
    "        entry[\"category\"] = category\n",
    "        entry[\"tags\"] = tags\n",
    "        return page_id, entry\n",
    "    return page_id, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29abe446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using max_workers = 12\n"
     ]
    }
   ],
   "source": [
    "physical_cores = multiprocessing.cpu_count() // 2\n",
    "max_threads = min(physical_cores * 2, 32)\n",
    "\n",
    "print(f\"[INFO] Using max_workers = {max_threads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Adding Gemma metadata to: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 735552.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: A-History-Of-The-Philippines.json\n",
      "\n",
      "[INFO] Adding Gemma metadata to: Arkitekturang-Filipino.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Arkitekturang-Filipino.json:   3%|▎         | 21/623 [03:29<1:31:59,  9.17s/it]"
     ]
    }
   ],
   "source": [
    "# Apply classification to all pages\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        path = os.path.join(data_dir, filename)\n",
    "        print(f\"\\n[INFO] Adding Gemma metadata to: {filename}\")\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            doc = json.load(f)\n",
    "\n",
    "        # Multithreading for pages in the doc\n",
    "        with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "            futures = [executor.submit(classify_and_update, pid, doc[pid]) for pid in doc]\n",
    "            for future in tqdm(futures, desc=f\"Classifying {filename}\"):\n",
    "                page_id, result = future.result()\n",
    "                if result:\n",
    "                    doc[page_id] = result\n",
    "\n",
    "        # Save updated doc\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Updated and saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f96891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ralfh\\AppData\\Local\\Temp\\ipykernel_8632\\826302932.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Dense Embeddings (BGE-M3)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e892a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c45b4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 9798.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Arkitekturang-Filipino.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Arkitekturang-Filipino.json: 100%|██████████| 623/623 [00:00<00:00, 11631.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Culture-And-Customs-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Culture-And-Customs-Of-The-Philippines.json: 100%|██████████| 273/273 [00:00<00:00, 10829.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Filipino-Politics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Filipino-Politics.json: 100%|██████████| 379/379 [00:00<00:00, 10024.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Food-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Food-Of-The-Philippines.json: 100%|██████████| 89/89 [00:00<00:00, 10885.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: History-Of-The-Flipino-People.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking History-Of-The-Flipino-People.json: 100%|██████████| 645/645 [00:00<00:00, 10138.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Philippine-History-Source-Book.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Philippine-History-Source-Book.json: 100%|██████████| 643/643 [00:00<00:00, 6906.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Philippine-Myths-Legends-And-Folktales.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Philippine-Myths-Legends-And-Folktales.json: 100%|██████████| 148/148 [00:00<00:00, 31391.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Sarap-Essays-On-Philippine-Food.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Sarap-Essays-On-Philippine-Food.json: 100%|██████████| 234/234 [00:00<00:00, 9252.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total chunks: 39369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "metadatas = []\n",
    "\n",
    "# Loop through each cleaned JSON file\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        print(f\"[INFO] Chunking file: {filename}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            doc = json.load(f)\n",
    "\n",
    "        for key, entry in tqdm(doc.items(), desc=f\"Chunking {filename}\"):\n",
    "            text = entry.get(\"content\", \"\")\n",
    "            if not text.strip():\n",
    "                continue\n",
    "\n",
    "            split_chunks = splitter.split_text(text)\n",
    "\n",
    "            for i, chunk in enumerate(split_chunks):\n",
    "\n",
    "                if len(chunk.split()) <= 10:\n",
    "                    continue\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"source\": filename,\n",
    "                    \"page\": entry.get(\"page\", key),\n",
    "                    \"chunk_id\": f\"{key}_chunk_{i}\",\n",
    "                    \"filename\": filename\n",
    "                })\n",
    "\n",
    "print(f\"[INFO] Total chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f62001b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma Vector Store (Dense Retrieval)\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    metadatas=metadatas,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"filipino_culture\"\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23fd0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks)\n",
    "bm25_retriever.k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ea1b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        bm25_retriever\n",
    "    ],\n",
    "    weights=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dc38cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, docs, top_n=3, verbose=False):\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    normalized_scores = [score / (len(doc.page_content.split()) + 1) for score, doc in zip(scores, docs)]\n",
    "    scored_docs = list(zip(normalized_scores, docs))\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        for i, (score, doc) in enumerate(scored_docs[:top_n], start=1):\n",
    "            print(f\"\\nRank {i} Score: {score:.4f}\")\n",
    "            print(doc.page_content[:300] + \"...\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    return scored_docs[:top_n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3038e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_data(\n",
    "    file_path: str, \n",
    "    randomize: bool = False, \n",
    "    limit: Optional[int] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if randomize:\n",
    "        random.shuffle(data)\n",
    "\n",
    "    if limit is not None:\n",
    "        data = data[:limit]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7224d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(a: str, b: str) -> float:\n",
    "    a_tokens = set(a.lower().split())\n",
    "    b_tokens = set(b.lower().split())\n",
    "\n",
    "    if not a_tokens or not b_tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = a_tokens.intersection(b_tokens)\n",
    "    union = a_tokens.union(b_tokens)\n",
    "    \n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e18c996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant(ground_truth: str, doc_content: str, threshold: float = 50, jaccard_threshold: float = 0.3) -> bool:\n",
    "    ground_truth = ground_truth.lower().strip()\n",
    "    doc_content = doc_content.lower().strip()\n",
    "\n",
    "    # Exact substring match\n",
    "    if ground_truth in doc_content or doc_content in ground_truth:\n",
    "        return True\n",
    "\n",
    "    # Bi-directional fuzzy match\n",
    "    similarity_1 = fuzz.partial_ratio(ground_truth, doc_content)\n",
    "    similarity_2 = fuzz.partial_ratio(doc_content, ground_truth)\n",
    "    if max(similarity_1, similarity_2) >= threshold:\n",
    "        return True\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    jaccard = jaccard_similarity(ground_truth, doc_content)\n",
    "    if jaccard >= jaccard_threshold:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(\n",
    "    eval_data: List[Dict[str, Any]],\n",
    "    retriever: Any,\n",
    "    reranker: Any,\n",
    "    k: int = 3,\n",
    "    fuzzy_threshold: float = 50,\n",
    "    doc_content_key: str = 'page_content'\n",
    ") -> Dict[str, float]:\n",
    "    hits = 0       # Queries with at least one relevant document\n",
    "    relevant_docs_total = 0\n",
    "    retrieved_docs_total = 0\n",
    "    reciprocal_ranks = []  \n",
    "    results_log = []\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    date_str = datetime.datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "    output_path = f\"results/result-{date_str}.json\"\n",
    "\n",
    "    for sample in tqdm(eval_data, desc=\"Evaluating\"):\n",
    "        question = sample[\"question\"]\n",
    "        ground_truth = sample[\"answer\"]\n",
    "\n",
    "        try:\n",
    "            # Retrieve initial documents\n",
    "            initial_results = retriever.get_relevant_documents(question)\n",
    "            if not initial_results:\n",
    "                reciprocal_ranks.append(0)\n",
    "                continue\n",
    "\n",
    "            # Rerank\n",
    "            reranked = reranker(question, initial_results, top_n=k)\n",
    "            if not reranked:\n",
    "                reciprocal_ranks.append(0)\n",
    "                continue\n",
    "\n",
    "            # Track relevance\n",
    "            found = False\n",
    "            retrieved_docs_total += len(reranked)\n",
    "            top_k_docs = []\n",
    "\n",
    "            for rank, (score, doc) in enumerate(reranked[:k]):\n",
    "                # Extract document content dynamically\n",
    "                doc_content = getattr(doc, doc_content_key, doc) if isinstance(doc, object) else doc\n",
    "                if isinstance(doc_content, dict):\n",
    "                    doc_content = doc_content.get('content', '')\n",
    "\n",
    "                top_k_docs.append({\n",
    "                    \"rank\": rank + 1,\n",
    "                    \"score\": round(score, 4),\n",
    "                    \"doc_content\": doc_content.strip()[:500]\n",
    "                })\n",
    "\n",
    "                # Check relevance\n",
    "                if is_relevant(ground_truth, doc_content, fuzzy_threshold):\n",
    "                    relevant_docs_total += 1\n",
    "                    if not found:\n",
    "                        hits += 1\n",
    "                        reciprocal_ranks.append(1 / (rank + 1))\n",
    "                        found = True\n",
    "\n",
    "            if not found:\n",
    "                reciprocal_ranks.append(0)\n",
    "\n",
    "            results_log.append({\n",
    "                \"query\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"top_k\": top_k_docs\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question '{question}': {e}\")\n",
    "            reciprocal_ranks.append(0)\n",
    "            results_log.append({\n",
    "                \"query\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"doc_content\": f\"[ERROR] {str(e)}\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "    # Save to results JSON\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results_log, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n[INFO] Results saved to {output_path}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    total_queries = len(eval_data)\n",
    "    recall = hits / total_queries if total_queries > 0 else 0.0\n",
    "    precision = relevant_docs_total / retrieved_docs_total if retrieved_docs_total > 0 else 0.0\n",
    "    mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "    return {\n",
    "        f\"Recall@{k}\": recall,\n",
    "        f\"Precision@{k}\": precision,\n",
    "        f\"MRR@{k}\": mrr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be369b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:23<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Results saved to results/result-06-25-2025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_data = load_eval_data(\n",
    "    \"evaluation.json\",\n",
    "    randomize=False,\n",
    "    limit=500 # Adjust as needed\n",
    ")\n",
    "\n",
    "results = evaluate_retriever(\n",
    "    eval_data=eval_data,\n",
    "    retriever=ensemble_retriever,\n",
    "    reranker=rerank_with_cross_encoder,\n",
    "    k=3,\n",
    "    fuzzy_threshold=50,\n",
    "    doc_content_key='page_content'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ce91eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Recall@3: 0.6400\n",
      "Precision@3: 0.4533\n",
      "MRR@3: 0.5767\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "427e5e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 | Score: 0.2302\n",
      "------------------------------------------------------------\n",
      ". This method, known as kinilaw, also makes use of the abundant seafood of the country and adds variety to the Filipinos  taste choices\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 2 | Score: 0.1502\n",
      "------------------------------------------------------------\n",
      ". In the days be- fore refrigeration became commonplace, the adobo served as a delicious way of preserving food. Kinilaw is another preserving process that has produced an appetizing dish that is the Filipino version of the Spanish seviche\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Output\n",
    "query = \"What is the traditional Filipino dish ‘kinilaw’?\"\n",
    "\n",
    "# Retrieve documents using hybrid retriever\n",
    "initial_results = ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "# Rerank using Cross-Encoder\n",
    "reranked = rerank_with_cross_encoder(query, initial_results, top_n=2)\n",
    "\n",
    "for i, (score, doc) in enumerate(reranked, start=1):\n",
    "    print(f\"\\nRank {i} | Score: {score:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(doc.page_content.strip()[:300])\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
