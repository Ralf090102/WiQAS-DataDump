{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb57e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import fitz\n",
    "import json\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"knowledge_base\"\n",
    "output_dir = \"data\"\n",
    "result_dir = \"results\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"[INFO] Created directory: {output_dir}\")\n",
    "\n",
    "# Create result directory if it doesn't exist\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "    print(f\"[INFO] Created directory: {result_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping A-History-Of-The-Philippines.pdf – already converted.\n",
      "[INFO] Skipping Arkitekturang-Filipino.pdf – already converted.\n",
      "[INFO] Skipping Culture-And-Customs-Of-The-Philippines.pdf – already converted.\n",
      "[INFO] Skipping Filipino-Politics.pdf – already converted.\n",
      "[INFO] Skipping Food-Of-The-Philippines.pdf – already converted.\n",
      "[INFO] Skipping History-Of-The-Flipino-People.pdf – already converted.\n",
      "[INFO] Skipping Philippine-History-Source-Book.pdf – already converted.\n",
      "[INFO] Skipping Philippine-Myths-Legends-And-Folktales.pdf – already converted.\n",
      "[INFO] Skipping Tikim-Essays-On-Philippine-Food.pdf – already converted.\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(input_dir):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(input_dir, filename)\n",
    "        json_filename = os.path.splitext(filename)[0] + \".json\"\n",
    "        json_path = os.path.join(output_dir, json_filename)\n",
    "\n",
    "        # Skip if already converted\n",
    "        if os.path.exists(json_path):\n",
    "            print(f\"[INFO] Skipping {filename} – already converted.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Extracting: {filename}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        data = {}\n",
    "\n",
    "        for page_number in tqdm(range(len(doc)), desc=f\"Processing {filename}\"):\n",
    "            page = doc[page_number]\n",
    "            text = page.get_text().strip()\n",
    "            if text:\n",
    "                data[f\"page_{page_number + 1}\"] = {\n",
    "                    \"page\": page_number + 1,\n",
    "                    \"content\": text\n",
    "                }\n",
    "\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Saved to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def clean_text(text):\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix ellipses or multiple punctuation\n",
    "    text = re.sub(r'\\.{3,}', '.', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    # Remove stray characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae214a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 7382.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: A-History-Of-The-Philippines.json\n",
      "[INFO] Preprocessing: Arkitekturang-Filipino.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Arkitekturang-Filipino.json: 100%|██████████| 623/623 [00:00<00:00, 8508.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Arkitekturang-Filipino.json\n",
      "[INFO] Preprocessing: Culture-And-Customs-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Culture-And-Customs-Of-The-Philippines.json: 100%|██████████| 273/273 [00:00<00:00, 7453.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Culture-And-Customs-Of-The-Philippines.json\n",
      "[INFO] Preprocessing: Filipino-Politics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Filipino-Politics.json: 100%|██████████| 379/379 [00:00<00:00, 4866.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Filipino-Politics.json\n",
      "[INFO] Preprocessing: Food-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Food-Of-The-Philippines.json: 100%|██████████| 89/89 [00:00<00:00, 7898.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Food-Of-The-Philippines.json\n",
      "[INFO] Preprocessing: History-Of-The-Flipino-People.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning History-Of-The-Flipino-People.json: 100%|██████████| 645/645 [00:00<00:00, 7223.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: History-Of-The-Flipino-People.json\n",
      "[INFO] Preprocessing: Philippine-History-Source-Book.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Philippine-History-Source-Book.json: 100%|██████████| 643/643 [00:00<00:00, 6684.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Philippine-History-Source-Book.json\n",
      "[INFO] Preprocessing: Philippine-Myths-Legends-And-Folktales.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Philippine-Myths-Legends-And-Folktales.json: 100%|██████████| 148/148 [00:00<00:00, 12998.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Philippine-Myths-Legends-And-Folktales.json\n",
      "[INFO] Preprocessing: Tikim-Essays-On-Philippine-Food.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Tikim-Essays-On-Philippine-Food.json: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Tikim-Essays-On-Philippine-Food.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "# Process all JSON files\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.lower().endswith(\".json\"):\n",
    "        json_path = os.path.join(data_dir, filename)\n",
    "        print(f\"[INFO] Preprocessing: {filename}\")\n",
    "        \n",
    "        # Load existing data\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Clean text for each page\n",
    "        for key in tqdm(data, desc=f\"Cleaning {filename}\"):\n",
    "            if \"content\" in data[key]:\n",
    "                data[key][\"content\"] = clean_text(data[key][\"content\"])\n",
    "        \n",
    "        # Overwrite file\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"[INFO] Finished cleaning: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f96891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_17508\\826302932.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Dense Embeddings (BGE-M3)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e892a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c45b4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 6626.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Arkitekturang-Filipino.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Arkitekturang-Filipino.json: 100%|██████████| 623/623 [00:00<00:00, 8685.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Culture-And-Customs-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Culture-And-Customs-Of-The-Philippines.json: 100%|██████████| 273/273 [00:00<00:00, 7934.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Filipino-Politics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Filipino-Politics.json: 100%|██████████| 379/379 [00:00<00:00, 5143.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Food-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Food-Of-The-Philippines.json: 100%|██████████| 89/89 [00:00<00:00, 10941.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: History-Of-The-Flipino-People.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking History-Of-The-Flipino-People.json: 100%|██████████| 645/645 [00:00<00:00, 7686.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Philippine-History-Source-Book.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Philippine-History-Source-Book.json: 100%|██████████| 643/643 [00:00<00:00, 6555.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Philippine-Myths-Legends-And-Folktales.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Philippine-Myths-Legends-And-Folktales.json: 100%|██████████| 148/148 [00:00<00:00, 30274.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Tikim-Essays-On-Philippine-Food.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Tikim-Essays-On-Philippine-Food.json: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total chunks: 36709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "metadatas = []\n",
    "\n",
    "# Loop through each cleaned JSON file\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        print(f\"[INFO] Chunking file: {filename}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            doc = json.load(f)\n",
    "\n",
    "        for key, entry in tqdm(doc.items(), desc=f\"Chunking {filename}\"):\n",
    "            text = entry.get(\"content\", \"\")\n",
    "            if not text.strip():\n",
    "                continue\n",
    "\n",
    "            split_chunks = splitter.split_text(text)\n",
    "\n",
    "            for i, chunk in enumerate(split_chunks):\n",
    "\n",
    "                if len(chunk.split()) <= 10:\n",
    "                    continue\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"source\": filename,\n",
    "                    \"page\": entry.get(\"page\", key),\n",
    "                    \"chunk_id\": f\"{key}_chunk_{i}\",\n",
    "                    \"filename\": filename\n",
    "                })\n",
    "\n",
    "print(f\"[INFO] Total chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62001b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma Vector Store (Dense Retrieval)\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    metadatas=metadatas,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"filipino_culture\"\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23fd0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks)\n",
    "bm25_retriever.k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea1b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        bm25_retriever\n",
    "    ],\n",
    "    weights=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc38cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, docs, top_n=3, verbose=False):\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    normalized_scores = [score / (len(doc.page_content.split()) + 1) for score, doc in zip(scores, docs)]\n",
    "    scored_docs = list(zip(normalized_scores, docs))\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        for i, (score, doc) in enumerate(scored_docs[:top_n], start=1):\n",
    "            print(f\"\\nRank {i} Score: {score:.4f}\")\n",
    "            print(doc.page_content[:300] + \"...\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    return scored_docs[:top_n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3038e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7224d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(a: str, b: str) -> float:\n",
    "    a_tokens = set(a.lower().split())\n",
    "    b_tokens = set(b.lower().split())\n",
    "\n",
    "    if not a_tokens or not b_tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = a_tokens.intersection(b_tokens)\n",
    "    union = a_tokens.union(b_tokens)\n",
    "    \n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18c996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant(ground_truth: str, doc_content: str, threshold: float = 50, jaccard_threshold: float = 0.3) -> bool:\n",
    "    ground_truth = ground_truth.lower().strip()\n",
    "    doc_content = doc_content.lower().strip()\n",
    "\n",
    "    # Exact substring match\n",
    "    if ground_truth in doc_content or doc_content in ground_truth:\n",
    "        return True\n",
    "\n",
    "    # Bi-directional fuzzy match\n",
    "    similarity_1 = fuzz.partial_ratio(ground_truth, doc_content)\n",
    "    similarity_2 = fuzz.partial_ratio(doc_content, ground_truth)\n",
    "    if max(similarity_1, similarity_2) >= threshold:\n",
    "        return True\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    jaccard = jaccard_similarity(ground_truth, doc_content)\n",
    "    if jaccard >= jaccard_threshold:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d1d0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(\n",
    "    eval_data: List[Dict[str, Any]],\n",
    "    retriever: Any,\n",
    "    reranker: Any,\n",
    "    k: int = 3,\n",
    "    fuzzy_threshold: float = 50,\n",
    "    doc_content_key: str = 'page_content'\n",
    ") -> Dict[str, float]:\n",
    "    hits = 0       # Queries with at least one relevant document\n",
    "    relevant_docs_total = 0\n",
    "    retrieved_docs_total = 0\n",
    "    reciprocal_ranks = []  \n",
    "    results_log = []\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    date_str = datetime.datetime.now().strftime(\"%m-%d-%Y\")\n",
    "    output_path = f\"results/result-{date_str}.json\"\n",
    "\n",
    "    for sample in tqdm(eval_data, desc=\"Evaluating\"):\n",
    "        question = sample[\"question\"]\n",
    "        ground_truth = sample[\"answer\"]\n",
    "\n",
    "        try:\n",
    "            # Retrieve initial documents\n",
    "            initial_results = retriever.get_relevant_documents(question)\n",
    "            if not initial_results:\n",
    "                reciprocal_ranks.append(0)\n",
    "                continue\n",
    "\n",
    "            # Rerank\n",
    "            reranked = reranker(question, initial_results, top_n=k)\n",
    "            if not reranked:\n",
    "                reciprocal_ranks.append(0)\n",
    "                continue\n",
    "\n",
    "            # Track relevance\n",
    "            found = False\n",
    "            retrieved_docs_total += len(reranked)\n",
    "\n",
    "             # Log top result\n",
    "            top_doc = reranked[0][1]\n",
    "            top_content = getattr(top_doc, doc_content_key, top_doc)\n",
    "            if isinstance(top_content, dict):\n",
    "                top_content = top_content.get('content', '')\n",
    "\n",
    "            for rank, (score, doc) in enumerate(reranked[:k]):\n",
    "                # Extract document content dynamically\n",
    "                doc_content = getattr(doc, doc_content_key, doc) if isinstance(doc, object) else doc\n",
    "                if isinstance(doc_content, dict):\n",
    "                    doc_content = doc_content.get('content', '')\n",
    "\n",
    "                # Check relevance\n",
    "                if is_relevant(ground_truth, doc_content, fuzzy_threshold):\n",
    "                    relevant_docs_total += 1\n",
    "                    if not found:\n",
    "                        hits += 1\n",
    "                        reciprocal_ranks.append(1 / (rank + 1))\n",
    "                        found = True\n",
    "\n",
    "            if not found:\n",
    "                reciprocal_ranks.append(0)\n",
    "\n",
    "            results_log.append({\n",
    "                \"query\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"doc_content\": top_content.strip()\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question '{question}': {e}\")\n",
    "            reciprocal_ranks.append(0)\n",
    "            results_log.append({\n",
    "                \"query\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"doc_content\": f\"[ERROR] {str(e)}\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "    # Save to results JSON\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results_log, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n[INFO] Results saved to {output_path}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    total_queries = len(eval_data)\n",
    "    recall = hits / total_queries if total_queries > 0 else 0.0\n",
    "    precision = relevant_docs_total / retrieved_docs_total if retrieved_docs_total > 0 else 0.0\n",
    "    mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "    return {\n",
    "        f\"Recall@{k}\": recall,\n",
    "        f\"Precision@{k}\": precision,\n",
    "        f\"MRR@{k}\": mrr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be369b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_17508\\3228794555.py:25: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  initial_results = retriever.get_relevant_documents(question)\n",
      "Evaluating: 100%|██████████| 200/200 [01:46<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Results saved to results/result-06-23-2025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_data = load_eval_data(\"evaluation/evaluation200.json\")\n",
    "\n",
    "results = evaluate_retriever(\n",
    "    eval_data=eval_data,\n",
    "    retriever=ensemble_retriever,\n",
    "    reranker=rerank_with_cross_encoder,\n",
    "    k=3,\n",
    "    fuzzy_threshold=50,\n",
    "    doc_content_key='page_content'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce91eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Recall@3: 0.6700\n",
      "Precision@3: 0.4133\n",
      "MRR@3: 0.5567\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e5e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 | Score: 0.3078\n",
      "------------------------------------------------------------\n",
      ". Kinilaw is another preserving process that has produced an appetizing dish that is the Filipino version of the Spanish seviche\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 2 | Score: 0.2302\n",
      "------------------------------------------------------------\n",
      ". This method, known as kinilaw, also makes use of the abundant seafood of the country and adds variety to the Filipinos  taste choices\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Output\n",
    "query = \"What is the traditional Filipino dish ‘kinilaw’?\"\n",
    "\n",
    "# Retrieve documents using hybrid retriever\n",
    "initial_results = ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "# Rerank using Cross-Encoder\n",
    "reranked = rerank_with_cross_encoder(query, initial_results, top_n=2)\n",
    "\n",
    "for i, (score, doc) in enumerate(reranked, start=1):\n",
    "    print(f\"\\nRank {i} | Score: {score:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(doc.page_content.strip()[:300])\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
