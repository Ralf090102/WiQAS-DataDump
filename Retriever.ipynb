{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb57e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import fitz\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import datetime\n",
    "import requests\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"knowledge_base\"\n",
    "output_dir = \"data\"\n",
    "result_dir = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"[INFO] Created directory: {output_dir}\")\n",
    "\n",
    "# Create result directory if it doesn't exist\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "    print(f\"[INFO] Created directory: {result_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping A-History-Of-The-Philippines.pdf – already converted.\n",
      "[INFO] Skipping Arkitekturang-Filipino.pdf – already converted.\n",
      "[INFO] Skipping Culture-And-Customs-Of-The-Philippines.pdf – already converted.\n",
      "[INFO] Skipping Filipino-Politics.pdf – already converted.\n",
      "[INFO] Skipping Food-Of-The-Philippines.pdf – already converted.\n",
      "[INFO] Skipping History-Of-The-Flipino-People.pdf – already converted.\n",
      "[INFO] Skipping Philippine-History-Source-Book.pdf – already converted.\n",
      "[INFO] Skipping Philippine-Myths-Legends-And-Folktales.pdf – already converted.\n",
      "[INFO] Skipping Sarap-Essays-On-Philippine-Food.pdf – already converted.\n"
     ]
    }
   ],
   "source": [
    "#Text extraction from PDF files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(input_dir, filename)\n",
    "        json_filename = os.path.splitext(filename)[0] + \".json\"\n",
    "        json_path = os.path.join(output_dir, json_filename)\n",
    "\n",
    "        # Skip if already converted\n",
    "        if os.path.exists(json_path):\n",
    "            print(f\"[INFO] Skipping {filename} – already converted.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Extracting: {filename}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        data = {}\n",
    "\n",
    "        for page_number in tqdm(range(len(doc)), desc=f\"Processing {filename}\"):\n",
    "            page = doc[page_number]\n",
    "            text = page.get_text().strip()\n",
    "            if text:\n",
    "                data[f\"page_{page_number + 1}\"] = {\n",
    "                    \"page\": page_number + 1,\n",
    "                    \"content\": text\n",
    "                }\n",
    "\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Saved to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def clean_text(text):\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix ellipses or multiple punctuation\n",
    "    text = re.sub(r'\\.{3,}', '.', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    # Remove stray characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae214a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 8139.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: A-History-Of-The-Philippines.json\n",
      "[INFO] Preprocessing: Arkitekturang-Filipino.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Arkitekturang-Filipino.json: 100%|██████████| 623/623 [00:00<00:00, 8998.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Arkitekturang-Filipino.json\n",
      "[INFO] Preprocessing: Culture-And-Customs-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Culture-And-Customs-Of-The-Philippines.json: 100%|██████████| 273/273 [00:00<00:00, 7908.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Culture-And-Customs-Of-The-Philippines.json\n",
      "[INFO] Preprocessing: Filipino-Politics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Filipino-Politics.json: 100%|██████████| 379/379 [00:00<00:00, 7196.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Filipino-Politics.json\n",
      "[INFO] Preprocessing: Food-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Food-Of-The-Philippines.json: 100%|██████████| 89/89 [00:00<00:00, 10849.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Food-Of-The-Philippines.json\n",
      "[INFO] Preprocessing: History-Of-The-Flipino-People.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning History-Of-The-Flipino-People.json: 100%|██████████| 645/645 [00:00<00:00, 8363.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: History-Of-The-Flipino-People.json\n",
      "[INFO] Preprocessing: Philippine-History-Source-Book.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Philippine-History-Source-Book.json: 100%|██████████| 643/643 [00:00<00:00, 7836.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Philippine-History-Source-Book.json\n",
      "[INFO] Preprocessing: Philippine-Myths-Legends-And-Folktales.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Philippine-Myths-Legends-And-Folktales.json: 100%|██████████| 148/148 [00:00<00:00, 14499.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Philippine-Myths-Legends-And-Folktales.json\n",
      "[INFO] Preprocessing: Sarap-Essays-On-Philippine-Food.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Sarap-Essays-On-Philippine-Food.json: 100%|██████████| 234/234 [00:00<00:00, 8545.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: Sarap-Essays-On-Philippine-Food.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "# Process all JSON files\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.lower().endswith(\".json\"):\n",
    "        json_path = os.path.join(data_dir, filename)\n",
    "        print(f\"[INFO] Preprocessing: {filename}\")\n",
    "        \n",
    "        # Load existing data\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Clean text for each page\n",
    "        for key in tqdm(data, desc=f\"Cleaning {filename}\"):\n",
    "            if \"content\" in data[key]:\n",
    "                data[key][\"content\"] = clean_text(data[key][\"content\"])\n",
    "        \n",
    "        # Overwrite file\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"[INFO] Finished cleaning: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3f7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "model_name = \"gemma3:latest\"\n",
    "ollama_url = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e802830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_gemma(text, model=model_name):\n",
    "    prompt = (\n",
    "        f\"Given the following page content:\\n\\n\\\"\\\"\\\"\\n{text.strip()[:1500]}\\n\\\"\\\"\\\"\\n\\n\"\n",
    "        \"Categorize this page broadly in one word, and generate 2 to 3 relevant tags. \"\n",
    "        \"Return only this format:\\n\"\n",
    "        \"Category: <category>\\nTags: <comma-separated tags>\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(ollama_url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            print(f\"[ERROR] Ollama responded with status {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ollama request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b20f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a single page\n",
    "def classify_and_update(page_id, entry):\n",
    "    text = entry.get(\"content\", \"\").strip()\n",
    "    if not text or \"category\" in entry:\n",
    "        return page_id, None\n",
    "    \n",
    "    response = classify_with_gemma(text)\n",
    "    if response:\n",
    "        lines = response.strip().split(\"\\n\")\n",
    "        category, tags = \"unknown\", []\n",
    "        for line in lines:\n",
    "            if line.lower().startswith(\"category:\"):\n",
    "                category = line.split(\":\", 1)[1].strip()\n",
    "            elif line.lower().startswith(\"tags:\"):\n",
    "                tags = [tag.strip() for tag in line.split(\":\", 1)[1].split(\",\")]\n",
    "        \n",
    "        entry[\"category\"] = category\n",
    "        entry[\"tags\"] = tags\n",
    "        return page_id, entry\n",
    "    return page_id, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abe446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using max_workers = 12\n"
     ]
    }
   ],
   "source": [
    "# Determine number of threads based on CPU cores\n",
    "physical_cores = multiprocessing.cpu_count() // 2\n",
    "max_threads = min(physical_cores * 2, 32)\n",
    "\n",
    "print(f\"[INFO] Using max_workers = {max_threads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf29181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Adding Gemma metadata to: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [46:35<00:00,  7.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: A-History-Of-The-Philippines.json\n",
      "\n",
      "[INFO] Adding Gemma metadata to: Arkitekturang-Filipino.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Arkitekturang-Filipino.json: 100%|██████████| 623/623 [1:17:29<00:00,  7.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: Arkitekturang-Filipino.json\n",
      "\n",
      "[INFO] Adding Gemma metadata to: Culture-And-Customs-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Culture-And-Customs-Of-The-Philippines.json: 100%|██████████| 273/273 [35:39<00:00,  7.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: Culture-And-Customs-Of-The-Philippines.json\n",
      "\n",
      "[INFO] Adding Gemma metadata to: Filipino-Politics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Filipino-Politics.json: 100%|██████████| 379/379 [52:26<00:00,  8.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: Filipino-Politics.json\n",
      "\n",
      "[INFO] Adding Gemma metadata to: Food-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Food-Of-The-Philippines.json: 100%|██████████| 89/89 [11:47<00:00,  7.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: Food-Of-The-Philippines.json\n",
      "\n",
      "[INFO] Adding Gemma metadata to: History-Of-The-Flipino-People.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying History-Of-The-Flipino-People.json: 100%|██████████| 645/645 [1:31:43<00:00,  8.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: History-Of-The-Flipino-People.json\n",
      "\n",
      "[INFO] Adding Gemma metadata to: Philippine-History-Source-Book.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Philippine-History-Source-Book.json: 100%|██████████| 643/643 [1:30:11<00:00,  8.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: Philippine-History-Source-Book.json\n",
      "\n",
      "[INFO] Adding Gemma metadata to: Philippine-Myths-Legends-And-Folktales.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Philippine-Myths-Legends-And-Folktales.json: 100%|██████████| 148/148 [20:55<00:00,  8.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: Philippine-Myths-Legends-And-Folktales.json\n",
      "\n",
      "[INFO] Adding Gemma metadata to: Sarap-Essays-On-Philippine-Food.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Sarap-Essays-On-Philippine-Food.json: 100%|██████████| 234/234 [38:04<00:00,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: Sarap-Essays-On-Philippine-Food.json\n"
     ]
    }
   ],
   "source": [
    "# Apply classification to all pages\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        path = os.path.join(data_dir, filename)\n",
    "        print(f\"\\n[INFO] Adding Gemma metadata to: {filename}\")\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            doc = json.load(f)\n",
    "\n",
    "        # Multithreading for pages in the doc\n",
    "        with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "            futures = [executor.submit(classify_and_update, pid, doc[pid]) for pid in doc]\n",
    "            for future in tqdm(futures, desc=f\"Classifying {filename}\"):\n",
    "                page_id, result = future.result()\n",
    "                if result:\n",
    "                    doc[page_id] = result\n",
    "\n",
    "        # Save updated doc\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Updated and saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f96891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_3828\\826302932.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Dense Embeddings (BGE-M3)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e892a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c45b4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 1216.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Arkitekturang-Filipino.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Arkitekturang-Filipino.json: 100%|██████████| 623/623 [00:00<00:00, 11901.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Culture-And-Customs-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Culture-And-Customs-Of-The-Philippines.json: 100%|██████████| 273/273 [00:00<00:00, 11404.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Filipino-Politics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Filipino-Politics.json: 100%|██████████| 379/379 [00:00<00:00, 10517.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Food-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Food-Of-The-Philippines.json: 100%|██████████| 89/89 [00:00<00:00, 11712.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: History-Of-The-Flipino-People.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking History-Of-The-Flipino-People.json: 100%|██████████| 645/645 [00:00<00:00, 11117.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Philippine-History-Source-Book.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Philippine-History-Source-Book.json: 100%|██████████| 643/643 [00:00<00:00, 5131.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Philippine-Myths-Legends-And-Folktales.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Philippine-Myths-Legends-And-Folktales.json: 100%|██████████| 148/148 [00:00<00:00, 1413.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: Sarap-Essays-On-Philippine-Food.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Sarap-Essays-On-Philippine-Food.json: 100%|██████████| 234/234 [00:00<00:00, 10893.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total chunks: 38463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "metadatas = []\n",
    "tag_counter = Counter()\n",
    "\n",
    "# Loop through each cleaned JSON file\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        print(f\"[INFO] Chunking file: {filename}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:   \n",
    "            doc = json.load(f)\n",
    "\n",
    "        for key, entry in tqdm(doc.items(), desc=f\"Chunking {filename}\"):\n",
    "            text = entry.get(\"content\", \"\")\n",
    "            if not text.strip():\n",
    "                continue\n",
    "\n",
    "            split_chunks = splitter.split_text(text)\n",
    "\n",
    "            # Get and clean category\n",
    "            category = entry.get(\"category\", \"unknown\")\n",
    "            category = str(category).strip()\n",
    "\n",
    "            # Get and clean tags\n",
    "            raw_tags = entry.get(\"tags\", [])\n",
    "            if not isinstance(raw_tags, list):\n",
    "                raw_tags = [raw_tags]\n",
    "\n",
    "            tags = [str(tag).strip() for tag in raw_tags if isinstance(tag, (str, int, float, bool))]\n",
    "            tags = [tag for tag in tags if tag]\n",
    "            tag_counter.update(tags)\n",
    "\n",
    "            for i, chunk in enumerate(split_chunks):\n",
    "                if len(chunk.split()) <= 10:\n",
    "                    continue\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"source\": filename,\n",
    "                    \"page\": entry.get(\"page\", key),\n",
    "                    \"chunk_id\": f\"{key}_chunk_{i}\",\n",
    "                    \"filename\": filename,\n",
    "                    \"category\": category,\n",
    "                    \"tags\": \", \".join(tags)\n",
    "                })\n",
    "\n",
    "print(f\"[INFO] Total chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "846284d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch insert initialization\n",
    "batch_size = 256\n",
    "\n",
    "def embed_batch(batch_texts):\n",
    "    return embedding_model.embed_documents(batch_texts)\n",
    "\n",
    "batches = [(chunks[i:i+batch_size], metadatas[i:i+batch_size])\n",
    "           for i in range(0, len(chunks), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d0c6a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 151/151 [1:00:22<00:00, 23.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parallel embedding and collection\n",
    "embedded_batches = []\n",
    "with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "    futures = {executor.submit(embed_batch, texts): (texts, metas)\n",
    "               for texts, metas in batches}\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Embedding\"):\n",
    "        try:\n",
    "            embeddings = future.result()\n",
    "            texts, metas = futures[future]\n",
    "            embedded_batches.append((texts, metas, embeddings))\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to embed batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62001b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Successfully removed previous Chroma DB.\n"
     ]
    }
   ],
   "source": [
    "# Clear previous DB\n",
    "if os.path.exists(\"./chroma_db\"):\n",
    "    try:\n",
    "        if 'vectorstore' in locals():\n",
    "            del vectorstore\n",
    "            import gc\n",
    "            gc.collect()\n",
    "        shutil.rmtree(\"./chroma_db\")\n",
    "        print(\"[INFO] Successfully removed previous Chroma DB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not fully clean Chroma DB: {e}\")\n",
    "else:\n",
    "    print(\"[INFO] No existing Chroma DB to remove.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "462e46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_3828\\2034488148.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chroma\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"filipino_culture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to generate stable unique IDs per chunk\n",
    "def get_chunk_id(meta):\n",
    "    return f\"{meta['filename'].replace('.json','')}_{meta['page']}_{meta['chunk_id']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d537b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing into Chroma: 100%|██████████| 151/151 [1:17:38<00:00, 30.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished indexing Chroma vector store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Index using Chroma's internal .upsert\n",
    "for texts, metas, embeds in tqdm(embedded_batches, desc=\"Indexing into Chroma\"):\n",
    "    ids = [get_chunk_id(meta) for meta in metas]\n",
    "\n",
    "    vectorstore._collection.upsert(\n",
    "        ids=ids,\n",
    "        documents=texts,\n",
    "        embeddings=embeds,\n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "print(\"[INFO] Finished indexing Chroma vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3038e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_data(\n",
    "    file_path: str, \n",
    "    randomize: bool = False, \n",
    "    limit: Optional[int] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if randomize:\n",
    "        random.shuffle(data)\n",
    "\n",
    "    if limit is not None:\n",
    "        data = data[:limit]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7224d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(a: str, b: str) -> float:\n",
    "    a_tokens = set(a.lower().split())\n",
    "    b_tokens = set(b.lower().split())\n",
    "\n",
    "    if not a_tokens or not b_tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = a_tokens.intersection(b_tokens)\n",
    "    union = a_tokens.union(b_tokens)\n",
    "    \n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e18c996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant(ground_truth: str, doc_content: str, threshold: float = 50, jaccard_threshold: float = 0.3) -> bool:\n",
    "    ground_truth = ground_truth.lower().strip()\n",
    "    doc_content = doc_content.lower().strip()\n",
    "\n",
    "    # Exact substring match\n",
    "    if ground_truth in doc_content or doc_content in ground_truth:\n",
    "        return True\n",
    "\n",
    "    # Bi-directional fuzzy match\n",
    "    similarity_1 = fuzz.partial_ratio(ground_truth, doc_content)\n",
    "    similarity_2 = fuzz.partial_ratio(doc_content, ground_truth)\n",
    "    if max(similarity_1, similarity_2) >= threshold:\n",
    "        return True\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    jaccard = jaccard_similarity(ground_truth, doc_content)\n",
    "    if jaccard >= jaccard_threshold:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfea1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query_with_gemma(question: str) -> dict:\n",
    "    prompt = (\n",
    "        f\"Given the following question:\\n\\n\\\"{question}\\\"\\n\\n\"\n",
    "        \"Classify this question with:\\n\"\n",
    "        \"Category: <a broad category in one word>\\n\"\n",
    "        \"Tags: <comma-separated 2 to 3 relevant keywords>\"\n",
    "    )\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(ollama_url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        output = response.json()[\"response\"]\n",
    "        category, tags = \"unknown\", []\n",
    "        for line in output.strip().split(\"\\n\"):\n",
    "            if line.lower().startswith(\"category:\"):\n",
    "                category = line.split(\":\", 1)[1].strip().strip(\"*\")\n",
    "            elif line.lower().startswith(\"tags:\"):\n",
    "                tags = [t.strip().strip(\"*\") for t in line.split(\":\", 1)[1].split(\",\")]\n",
    "        return {\"category\": category, \"tags\": tags}\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to classify: {question}\")\n",
    "        return {\"category\": \"unknown\", \"tags\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cbc5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_embedding_match(tags: List[str], doc_tags: List[str], threshold: float = 0.6) -> bool:\n",
    "    if not tags or not doc_tags:\n",
    "        return False\n",
    "\n",
    "    query_embeds = [embedding_model.embed_query(tag) for tag in tags]\n",
    "    doc_embeds = [embedding_model.embed_query(tag) for tag in doc_tags]\n",
    "\n",
    "    sims = [cosine_similarity([q], [d])[0][0] for q in query_embeds for d in doc_embeds]\n",
    "    return max(sims, default=0.0) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_by_metadata(query: str,\n",
    "                         category: str,\n",
    "                         tags: List[str],\n",
    "                         top_k: int = 3,\n",
    "                         verbose=False,\n",
    "                         category_threshold: float = 0.6,\n",
    "                         tag_sim_threshold: float = 0.6) -> List[Any]:\n",
    "\n",
    "    category_embedding = embedding_model.embed_query(category)\n",
    "    candidate_docs = vectorstore.similarity_search(query=query, k=50)\n",
    "\n",
    "    filtered_docs = []\n",
    "    seen = set()\n",
    "\n",
    "    category_pass = 0\n",
    "    tag_pass = 0\n",
    "    total_skipped_no_meta = 0\n",
    "\n",
    "    for doc in candidate_docs:\n",
    "        doc_meta = doc.metadata or {}\n",
    "        doc_cat = doc_meta.get(\"category\", \"\").strip()\n",
    "        if not doc_cat:\n",
    "            total_skipped_no_meta += 1\n",
    "            continue\n",
    "\n",
    "        doc_cat_embedding = embedding_model.embed_query(doc_cat)\n",
    "        cat_sim = cosine_similarity([category_embedding], [doc_cat_embedding])[0][0]\n",
    "        if cat_sim < category_threshold:\n",
    "            continue\n",
    "        category_pass += 1\n",
    "\n",
    "        doc_tags = doc_meta.get(\"tags\", \"\")\n",
    "        if isinstance(doc_tags, str):\n",
    "            doc_tags = [t.strip() for t in doc_tags.split(\",\") if t.strip()]\n",
    "\n",
    "        if not tag_embedding_match(tags, doc_tags, threshold=tag_sim_threshold):\n",
    "            continue\n",
    "        tag_pass += 1\n",
    "\n",
    "        snippet = doc.page_content[:50].strip().lower()\n",
    "        if snippet in seen:\n",
    "            continue\n",
    "\n",
    "        seen.add(snippet)\n",
    "        filtered_docs.append(doc)\n",
    "        if len(filtered_docs) >= top_k:\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n[INFO] Retrieved: {len(candidate_docs)} Top: candidates for query: \\\"{query[:128]}...\\\"\")\n",
    "        print(f\"[INFO] Skipped (missing metadata): {total_skipped_no_meta}\")\n",
    "        print(f\"[INFO] Passed category filter: {category_pass}\")\n",
    "        print(f\"[INFO] Passed tag filter: {tag_pass}\")\n",
    "        print(f\"[INFO] Final top-k after deduplication: {len(filtered_docs)}\\n\")\n",
    "\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d1d0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(\n",
    "    eval_data: list,\n",
    "    reranker,\n",
    "    k=3,\n",
    "    fuzzy_threshold=70,\n",
    "    jaccard_threshold=0.4,\n",
    "    doc_content_key=\"page_content\"\n",
    "):\n",
    "    hits = 0\n",
    "    relevant_docs_total = 0\n",
    "    retrieved_docs_total = 0\n",
    "    reciprocal_ranks = []\n",
    "    results_flat = []\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    file_path = f\"result_{timestamp}\"\n",
    "    csv_path = f\"results/{file_path}.csv\"\n",
    "\n",
    "    tqdm_params = dict(\n",
    "        desc=\"Evaluating\",\n",
    "        dynamic_ncols=True,\n",
    "        file=sys.stdout,\n",
    "        leave=True,\n",
    "        mininterval=900.0   # 15 minutes\n",
    "    )\n",
    "\n",
    "    for idx, sample in enumerate(tqdm(eval_data, **tqdm_params)):\n",
    "        question = sample[\"question\"]\n",
    "        ground_truth = sample[\"answer\"]\n",
    "\n",
    "        classification = classify_query_with_gemma(question)\n",
    "        category = classification[\"category\"]\n",
    "        tags = classification[\"tags\"]\n",
    "\n",
    "        if not category or category.lower() == \"unknown\":\n",
    "            print(f\"[WARN] Question {idx+1} got weak category from Gemma: {question[:60]}...\")\n",
    "\n",
    "        try:\n",
    "            initial_docs = retrieve_by_metadata(question, category, tags, top_k=k, verbose=False)\n",
    "            if not initial_docs:\n",
    "                reciprocal_ranks.append(0)\n",
    "                results_flat.append({\n",
    "                    \"item\": idx + 1,\n",
    "                    \"query\": question,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                    \"category\": category,\n",
    "                    \"tags\": \", \".join(tags),\n",
    "                    \"rank\": 1,\n",
    "                    \"score\": \"N/A\",\n",
    "                    \"found\": False,\n",
    "                    \"doc_content\": \"[NO DOCUMENTS RETRIEVED]\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            reranked = reranker(question, initial_docs, top_n=k)\n",
    "            if not reranked:\n",
    "                reciprocal_ranks.append(0)\n",
    "                results_flat.append({\n",
    "                    \"item\": idx + 1,\n",
    "                    \"query\": question,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                    \"category\": category,\n",
    "                    \"tags\": \", \".join(tags),\n",
    "                    \"rank\": 1,\n",
    "                    \"score\": \"N/A\",\n",
    "                    \"found\": False,\n",
    "                    \"doc_content\": \"[NO DOCUMENTS RERANKED]\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            found = False\n",
    "            retrieved_docs_total += len(reranked)\n",
    "\n",
    "            for rank, (score, doc) in enumerate(reranked):\n",
    "                doc_content = getattr(doc, doc_content_key, doc)\n",
    "                if isinstance(doc_content, dict):\n",
    "                    doc_content = doc_content.get(\"content\", \"\")\n",
    "\n",
    "                if is_relevant(ground_truth, doc_content, fuzzy_threshold, jaccard_threshold):\n",
    "                    relevant_docs_total += 1\n",
    "                    if not found:\n",
    "                        hits += 1\n",
    "                        reciprocal_ranks.append(1 / (rank + 1))\n",
    "                        found = True\n",
    "\n",
    "                results_flat.append({\n",
    "                    \"item\": idx + 1,\n",
    "                    \"query\": question,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                    \"category\": category,\n",
    "                    \"tags\": \", \".join(tags),\n",
    "                    \"rank\": rank + 1,\n",
    "                    \"score\": f\"{score:.4f}\",\n",
    "                    \"found\": found,\n",
    "                    \"doc_content\": doc_content[:500]\n",
    "                })\n",
    "\n",
    "            # If nothing relevant was found, log rank 1 again with found=False\n",
    "            if not found:\n",
    "                reciprocal_ranks.append(0)\n",
    "                if reranked:\n",
    "                    score, doc = reranked[0]\n",
    "                    doc_content = getattr(doc, doc_content_key, doc)\n",
    "                    if isinstance(doc_content, dict):\n",
    "                        doc_content = doc_content.get(\"content\", \"\")\n",
    "\n",
    "                    results_flat.append({\n",
    "                        \"item\": idx + 1,\n",
    "                        \"query\": question,\n",
    "                        \"ground_truth\": ground_truth,\n",
    "                        \"category\": category,\n",
    "                        \"tags\": \", \".join(tags),\n",
    "                        \"rank\": 1,\n",
    "                        \"score\": f\"{score:.4f}\",\n",
    "                        \"found\": False,\n",
    "                        \"doc_content\": doc_content[:500]\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            reciprocal_ranks.append(0)\n",
    "            results_flat.append({\n",
    "                \"item\": idx + 1,\n",
    "                \"query\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"category\": category,\n",
    "                \"tags\": \", \".join(tags),\n",
    "                \"rank\": \"error\",\n",
    "                \"score\": \"N/A\",\n",
    "                \"found\": False,\n",
    "                \"doc_content\": f\"[ERROR] {str(e)}\"\n",
    "            })\n",
    "\n",
    "    # Save output\n",
    "    df = pd.DataFrame(results_flat)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\n[INFO] Saved CSV results to: {csv_path}\")\n",
    "\n",
    "    total_queries = len(eval_data)\n",
    "    metrics = {\n",
    "        f\"Recall@{k}\": hits / total_queries if total_queries > 0 else 0.0,\n",
    "        f\"Precision@{k}\": relevant_docs_total / retrieved_docs_total if retrieved_docs_total > 0 else 0.0,\n",
    "        f\"MRR@{k}\": np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "    }\n",
    "\n",
    "    return metrics, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4388300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, docs, top_n=3, verbose=False):\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    normalized_scores = [score / (len(doc.page_content.split()) + 1) for score, doc in zip(scores, docs)]\n",
    "    scored_docs = list(zip(normalized_scores, docs))\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        for i, (score, doc) in enumerate(scored_docs[:top_n], start=1):\n",
    "            print(f\"\\nRank {i} Score: {score:.4f}\")\n",
    "            print(doc.page_content[:300] + \"...\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    return scored_docs[:top_n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be369b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|█▌        | 119/750 [17:53<1:34:52,  9.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m eval_data = load_eval_data(\u001b[33m\"\u001b[39m\u001b[33mevaluation.json\u001b[39m\u001b[33m\"\u001b[39m, randomize=\u001b[38;5;28;01mFalse\u001b[39;00m, limit=\u001b[32m750\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results, file_path = \u001b[43mevaluate_retriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreranker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrerank_with_cross_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuzzy_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjaccard_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc_content_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpage_content\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mevaluate_retriever\u001b[39m\u001b[34m(eval_data, reranker, k, fuzzy_threshold, jaccard_threshold, doc_content_key)\u001b[39m\n\u001b[32m     29\u001b[39m question = sample[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     30\u001b[39m ground_truth = sample[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m classification = \u001b[43mclassify_query_with_gemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m category = classification[\u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     34\u001b[39m tags = classification[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mclassify_query_with_gemma\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      2\u001b[39m prompt = (\n\u001b[32m      3\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGiven the following question:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mClassify this question with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCategory: <a broad category in one word>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTags: <comma-separated 2 to 3 relevant keywords>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m payload = {\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model_name,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     12\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mollama_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     15\u001b[39m     output = response.json()[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "eval_data = load_eval_data(\"evaluation.json\", randomize=False, limit=750)\n",
    "\n",
    "results, file_path = evaluate_retriever(\n",
    "    eval_data=eval_data,\n",
    "    reranker=rerank_with_cross_encoder,\n",
    "    k=3,\n",
    "    fuzzy_threshold=60,\n",
    "    jaccard_threshold = 0.4,\n",
    "    doc_content_key='page_content'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce91eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Recall@3: 0.0053\n",
      "Precision@3: 0.0020\n",
      "MRR@3: 0.0044\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99b419",
   "metadata": {},
   "source": [
    "# View Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adec29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>found</th>\n",
       "      <th>doc_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Who is the national hero of the Philippines?</td>\n",
       "      <td>Jose Rizal</td>\n",
       "      <td>History</td>\n",
       "      <td>Philippines, Hero, Jose Rizal</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>True</td>\n",
       "      <td>. The nineteenth-century revolutionary General Artemio Ricarte once proposed naming the country the Rizaline Islands, after its foremost national hero, Jos Rizal (with Filipinos henceforth to be known as Rizalinos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Who is the national hero of the Philippines?</td>\n",
       "      <td>Jose Rizal</td>\n",
       "      <td>History</td>\n",
       "      <td>Philippines, Hero, Jose Rizal</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1244</td>\n",
       "      <td>True</td>\n",
       "      <td>nationwide revolution led by General Emilio Aguinaldo and the founder of the secret revolutionary society, the Katipunan, Andres Bonifacio, under the inspiration of the Filipino hero and martyr, Dr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Who is the national hero of the Philippines?</td>\n",
       "      <td>Jose Rizal</td>\n",
       "      <td>History</td>\n",
       "      <td>Philippines, Hero, Jose Rizal</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0774</td>\n",
       "      <td>True</td>\n",
       "      <td>President raise the Philippine national flag to rule supreme and sovereign over our land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Ano ang ibig sabihin ng ‘bayanihan’ sa kulturang Pilipino?</td>\n",
       "      <td>Ang bayanihan ay ang tradisyon ng pagtutulungan at pagkakaisa sa komunidad, tulad ng sama-samang paglilipat ng bahay-kubo.</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Filipino culture, bayanihan, community spirit</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0594</td>\n",
       "      <td>False</td>\n",
       "      <td>. Filipino s pre Spanish conceptualization for a superior being. However, Filipinos were not monotheistic and this term may only be that applied to the most powerful of a wider range of deities. Bayanihan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Ano ang ibig sabihin ng ‘bayanihan’ sa kulturang Pilipino?</td>\n",
       "      <td>Ang bayanihan ay ang tradisyon ng pagtutulungan at pagkakaisa sa komunidad, tulad ng sama-samang paglilipat ng bahay-kubo.</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Filipino culture, bayanihan, community spirit</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.2390</td>\n",
       "      <td>False</td>\n",
       "      <td>. The term is used to describe the audience for lowbrow entertainment or an artistic production that could only appeal to such a low level. It is most often applied to popular Filipino films. Balagtasan. Contest in poetic verse waged between debaters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item                                                       query                                                                                                                ground_truth category                                           tags  rank   score  found                                                                                                                                                                                                                                                 doc_content\n",
       "0     1                Who is the national hero of the Philippines?                                                                                                                  Jose Rizal  History                  Philippines, Hero, Jose Rizal     1  0.1760   True                                      . The nineteenth-century revolutionary General Artemio Ricarte once proposed naming the country the Rizaline Islands, after its foremost national hero, Jos Rizal (with Filipinos henceforth to be known as Rizalinos)\n",
       "1     1                Who is the national hero of the Philippines?                                                                                                                  Jose Rizal  History                  Philippines, Hero, Jose Rizal     2  0.1244   True                                                       nationwide revolution led by General Emilio Aguinaldo and the founder of the secret revolutionary society, the Katipunan, Andres Bonifacio, under the inspiration of the Filipino hero and martyr, Dr\n",
       "2     1                Who is the national hero of the Philippines?                                                                                                                  Jose Rizal  History                  Philippines, Hero, Jose Rizal     3 -0.0774   True                                                                                                                                                                    President raise the Philippine national flag to rule supreme and sovereign over our land\n",
       "3     2  Ano ang ibig sabihin ng ‘bayanihan’ sa kulturang Pilipino?  Ang bayanihan ay ang tradisyon ng pagtutulungan at pagkakaisa sa komunidad, tulad ng sama-samang paglilipat ng bahay-kubo.  Culture  Filipino culture, bayanihan, community spirit     1 -0.0594  False                                                . Filipino s pre Spanish conceptualization for a superior being. However, Filipinos were not monotheistic and this term may only be that applied to the most powerful of a wider range of deities. Bayanihan\n",
       "4     2  Ano ang ibig sabihin ng ‘bayanihan’ sa kulturang Pilipino?  Ang bayanihan ay ang tradisyon ng pagtutulungan at pagkakaisa sa komunidad, tulad ng sama-samang paglilipat ng bahay-kubo.  Culture  Filipino culture, bayanihan, community spirit     2 -0.2390  False  . The term is used to describe the audience for lowbrow entertainment or an artistic production that could only appeal to such a low level. It is most often applied to popular Filipino films. Balagtasan. Contest in poetic verse waged between debaters"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"results/{file_path}.csv\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951dcd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'html\\data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m html_dir = \u001b[33m\"\u001b[39m\u001b[33mhtml\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m os.makedirs(html_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhtml/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.html\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:3386\u001b[39m, in \u001b[36mDataFrame.to_html\u001b[39m\u001b[34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, bold_rows, classes, escape, notebook, border, table_id, render_links, encoding)\u001b[39m\n\u001b[32m   3366\u001b[39m formatter = fmt.DataFrameFormatter(\n\u001b[32m   3367\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3368\u001b[39m     columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3383\u001b[39m     show_dimensions=show_dimensions,\n\u001b[32m   3384\u001b[39m )\n\u001b[32m   3385\u001b[39m \u001b[38;5;66;03m# TODO: a generic formatter wld b in DataFrameFormatter\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnotebook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnotebook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mborder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mborder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtable_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrender_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrender_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3394\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:937\u001b[39m, in \u001b[36mDataFrameRenderer.to_html\u001b[39m\u001b[34m(self, buf, encoding, classes, notebook, border, table_id, render_links)\u001b[39m\n\u001b[32m    929\u001b[39m html_formatter = Klass(\n\u001b[32m    930\u001b[39m     \u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m    931\u001b[39m     classes=classes,\n\u001b[32m   (...)\u001b[39m\u001b[32m    934\u001b[39m     render_links=render_links,\n\u001b[32m    935\u001b[39m )\n\u001b[32m    936\u001b[39m string = html_formatter.to_string()\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msave_to_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1033\u001b[39m, in \u001b[36msave_to_buffer\u001b[39m\u001b[34m(string, buf, encoding)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_to_buffer\u001b[39m(\n\u001b[32m   1026\u001b[39m     string: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1027\u001b[39m     buf: FilePath | WriteBuffer[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1028\u001b[39m     encoding: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1029\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1030\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[33;03m    Perform serialization. Write to buf or return as string if buf is None.\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_get_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfd\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# error: \"WriteBuffer[str]\" has no attribute \"getvalue\"\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1064\u001b[39m, in \u001b[36m_get_buffer\u001b[39m\u001b[34m(buf, encoding)\u001b[39m\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m buf  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(buf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(buf, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=encoding, newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1066\u001b[39m         \u001b[38;5;66;03m# GH#30034 open instead of codecs.open prevents a file leak\u001b[39;00m\n\u001b[32m   1067\u001b[39m         \u001b[38;5;66;03m#  if we have an invalid encoding argument.\u001b[39;00m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;66;03m# newline=\"\" is needed to roundtrip correctly on\u001b[39;00m\n\u001b[32m   1069\u001b[39m         \u001b[38;5;66;03m#  windows test_to_latex_filename\u001b[39;00m\n\u001b[32m   1070\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Thesis\\DatasetDump\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'html\\data'"
     ]
    }
   ],
   "source": [
    "html_dir = \"html\"\n",
    "os.makedirs(html_dir, exist_ok=True)\n",
    "\n",
    "df.to_html(f\"html/{file_path}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837418c",
   "metadata": {},
   "source": [
    "# SINGLE QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Output\n",
    "query = \"What is the most famous Filipino dish?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b461f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Category: Food\n",
      "[INFO] Tags: ['Filipino cuisine', 'Adobo', 'Adobo recipe']\n"
     ]
    }
   ],
   "source": [
    "# Classify with Gemma\n",
    "classification = classify_query_with_gemma(query)\n",
    "category = classification[\"category\"]\n",
    "tags = classification[\"tags\"]\n",
    "\n",
    "print(f\"[INFO] Category: {category}\")\n",
    "print(f\"[INFO] Tags: {tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f18f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Retrieved: 50 Top: candidates for query: \"What is the most famous Filipino dish?...\"\n",
      "[INFO] Skipped (missing metadata): 0\n",
      "[INFO] Passed category filter: 12\n",
      "[INFO] Passed tag filter: 10\n",
      "[INFO] Final top-k after deduplication: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve documents by category and tags\n",
    "initial_docs = retrieve_by_metadata(query, category, tags, top_k=10, verbose=True)\n",
    "\n",
    "if not initial_docs:\n",
    "    print(\"[WARN] No documents found after filtering.\")\n",
    "else:\n",
    "    reranked = rerank_with_cross_encoder(query, initial_docs, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b66a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 | Score: 0.2081\n",
      "------------------------------------------------------------\n",
      "Why Sinigang? ,i ATHER than the overworked ado- bo (so often identified as the Philippine stew in foreign cookbooks), sinigang seems to me the dish most representative of Filipino taste\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 2 | Score: 0.1913\n",
      "------------------------------------------------------------\n",
      ". The Filipino dishes change with each menu revision, but include such dishes as Manok sa Gata, Batangas Adobo (liver, chicken and pork), Kalderetang Kambing and an authen tic Kari-karing Buntot\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 3 | Score: 0.1913\n",
      "------------------------------------------------------------\n",
      ". This is accomplished by salting meat and fish or marinating food in vinegar and spices to be eaten raw. Two well known Filipino dishes, adobo and sinigang, are prepared by stewing\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print output\n",
    "for i, (score, doc) in enumerate(reranked, start=1):\n",
    "    print(f\"\\nRank {i} | Score: {score:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(doc.page_content.strip()[:300])\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
