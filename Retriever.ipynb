{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebb57e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import fitz\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import datetime\n",
    "import requests\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"knowledge_base\"\n",
    "output_dir = \"data\"\n",
    "result_dir = \"results\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"[INFO] Created directory: {output_dir}\")\n",
    "\n",
    "# Create result directory if it doesn't exist\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "    print(f\"[INFO] Created directory: {result_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a51619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping A-History-Of-The-Philippines.pdf – already converted.\n"
     ]
    }
   ],
   "source": [
    "#Text extraction from PDF files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(input_dir, filename)\n",
    "        json_filename = os.path.splitext(filename)[0] + \".json\"\n",
    "        json_path = os.path.join(output_dir, json_filename)\n",
    "\n",
    "        # Skip if already converted\n",
    "        if os.path.exists(json_path):\n",
    "            print(f\"[INFO] Skipping {filename} – already converted.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Extracting: {filename}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        data = {}\n",
    "\n",
    "        for page_number in tqdm(range(len(doc)), desc=f\"Processing {filename}\"):\n",
    "            page = doc[page_number]\n",
    "            text = page.get_text().strip()\n",
    "            if text:\n",
    "                data[f\"page_{page_number + 1}\"] = {\n",
    "                    \"page\": page_number + 1,\n",
    "                    \"content\": text\n",
    "                }\n",
    "\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Saved to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "959709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def clean_text(text):\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix ellipses or multiple punctuation\n",
    "    text = re.sub(r'\\.{3,}', '.', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    # Remove stray characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae214a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 7955.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished cleaning: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "# Process all JSON files\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.lower().endswith(\".json\"):\n",
    "        json_path = os.path.join(data_dir, filename)\n",
    "        print(f\"[INFO] Preprocessing: {filename}\")\n",
    "        \n",
    "        # Load existing data\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Clean text for each page\n",
    "        for key in tqdm(data, desc=f\"Cleaning {filename}\"):\n",
    "            if \"content\" in data[key]:\n",
    "                data[key][\"content\"] = clean_text(data[key][\"content\"])\n",
    "        \n",
    "        # Overwrite file\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"[INFO] Finished cleaning: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e3f7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "model_name = \"gemma3:latest\"\n",
    "ollama_url = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e802830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_gemma(text, model=model_name):\n",
    "    prompt = (\n",
    "        f\"Given the following page content:\\n\\n\\\"\\\"\\\"\\n{text.strip()[:1500]}\\n\\\"\\\"\\\"\\n\\n\"\n",
    "        \"Categorize this page broadly in one word, and generate 2 to 3 relevant tags. \"\n",
    "        \"Return only this format:\\n\"\n",
    "        \"Category: <category>\\nTags: <comma-separated tags>\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(ollama_url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            print(f\"[ERROR] Ollama responded with status {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ollama request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2b20f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a single page\n",
    "def classify_and_update(page_id, entry):\n",
    "    text = entry.get(\"content\", \"\").strip()\n",
    "    if not text or \"category\" in entry:\n",
    "        return page_id, None\n",
    "    \n",
    "    response = classify_with_gemma(text)\n",
    "    if response:\n",
    "        lines = response.strip().split(\"\\n\")\n",
    "        category, tags = \"unknown\", []\n",
    "        for line in lines:\n",
    "            if line.lower().startswith(\"category:\"):\n",
    "                category = line.split(\":\", 1)[1].strip()\n",
    "            elif line.lower().startswith(\"tags:\"):\n",
    "                tags = [tag.strip() for tag in line.split(\":\", 1)[1].split(\",\")]\n",
    "        \n",
    "        entry[\"category\"] = category\n",
    "        entry[\"tags\"] = tags\n",
    "        return page_id, entry\n",
    "    return page_id, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29abe446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using max_workers = 12\n"
     ]
    }
   ],
   "source": [
    "physical_cores = multiprocessing.cpu_count() // 2\n",
    "max_threads = min(physical_cores * 2, 32)\n",
    "\n",
    "print(f\"[INFO] Using max_workers = {max_threads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf29181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Adding Gemma metadata to: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 1089906.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated and saved: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply classification to all pages\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        path = os.path.join(data_dir, filename)\n",
    "        print(f\"\\n[INFO] Adding Gemma metadata to: {filename}\")\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            doc = json.load(f)\n",
    "\n",
    "        # Multithreading for pages in the doc\n",
    "        with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "            futures = [executor.submit(classify_and_update, pid, doc[pid]) for pid in doc]\n",
    "            for future in tqdm(futures, desc=f\"Classifying {filename}\"):\n",
    "                page_id, result = future.result()\n",
    "                if result:\n",
    "                    doc[page_id] = result\n",
    "\n",
    "        # Save updated doc\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Updated and saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3f96891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Embeddings (BGE-M3)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e892a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c45b4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunking file: A-History-Of-The-Philippines.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking A-History-Of-The-Philippines.json: 100%|██████████| 356/356 [00:00<00:00, 10214.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total chunks: 4222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "metadatas = []\n",
    "tag_counter = Counter()\n",
    "\n",
    "# Loop through each cleaned JSON file\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        print(f\"[INFO] Chunking file: {filename}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:   \n",
    "            doc = json.load(f)\n",
    "\n",
    "        for key, entry in tqdm(doc.items(), desc=f\"Chunking {filename}\"):\n",
    "            text = entry.get(\"content\", \"\")\n",
    "            if not text.strip():\n",
    "                continue\n",
    "\n",
    "            split_chunks = splitter.split_text(text)\n",
    "\n",
    "            # Get and clean category\n",
    "            category = entry.get(\"category\", \"unknown\")\n",
    "            category = str(category).strip()\n",
    "\n",
    "            # Get and clean tags\n",
    "            raw_tags = entry.get(\"tags\", [])\n",
    "            if not isinstance(raw_tags, list):\n",
    "                raw_tags = [raw_tags]\n",
    "\n",
    "            tags = [str(tag).strip() for tag in raw_tags if isinstance(tag, (str, int, float, bool))]\n",
    "            tags = [tag for tag in tags if tag]\n",
    "            tag_counter.update(tags)\n",
    "\n",
    "            for i, chunk in enumerate(split_chunks):\n",
    "                if len(chunk.split()) <= 10:\n",
    "                    continue\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"source\": filename,\n",
    "                    \"page\": entry.get(\"page\", key),\n",
    "                    \"chunk_id\": f\"{key}_chunk_{i}\",\n",
    "                    \"filename\": filename,\n",
    "                    \"category\": category,\n",
    "                    \"tags\": \", \".join(tags)\n",
    "                })\n",
    "\n",
    "print(f\"[INFO] Total chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "846284d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch insert initialization\n",
    "batch_size = 256\n",
    "\n",
    "def embed_batch(batch_texts):\n",
    "    return embedding_model.embed_documents(batch_texts)\n",
    "\n",
    "batches = [(chunks[i:i+batch_size], metadatas[i:i+batch_size])\n",
    "           for i in range(0, len(chunks), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d0c6a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 17/17 [06:18<00:00, 22.25s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Parallel embedding and collection\n",
    "embedded_batches = []\n",
    "with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "    futures = {executor.submit(embed_batch, texts): (texts, metas)\n",
    "               for texts, metas in batches}\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Embedding\"):\n",
    "        try:\n",
    "            embeddings = future.result()\n",
    "            texts, metas = futures[future]\n",
    "            embedded_batches.append((texts, metas, embeddings))\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to embed batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f62001b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Could not fully clean Chroma DB: [WinError 32] The process cannot access the file because it is being used by another process: './chroma_db\\\\bf452668-cc3a-4df7-bc04-9e69d0f8f8e0\\\\data_level0.bin'\n"
     ]
    }
   ],
   "source": [
    "# Clear previous DB\n",
    "if os.path.exists(\"./chroma_db\"):\n",
    "    try:\n",
    "        # Delete existing vectorstore if already defined\n",
    "        if 'vectorstore' in locals():\n",
    "            del vectorstore\n",
    "            gc.collect()\n",
    "\n",
    "        shutil.rmtree(\"./chroma_db\")\n",
    "        print(\"[INFO] Successfully removed previous Chroma DB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not fully clean Chroma DB: {e}\")\n",
    "else:\n",
    "    print(\"[INFO] No existing Chroma DB to remove.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "462e46af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"filipino_culture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d537b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing into Chroma: 100%|██████████| 17/17 [07:54<00:00, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished indexing Chroma vector store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add batches to vectorstore\n",
    "for texts, metas, embeds in tqdm(embedded_batches, desc=\"Indexing into Chroma\"):\n",
    "    vectorstore.add_texts(texts=texts, metadatas=metas, embeddings=embeds)\n",
    "\n",
    "print(\"[INFO] Finished indexing Chroma vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23fd0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks)\n",
    "bm25_retriever.k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3038e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_data(\n",
    "    file_path: str, \n",
    "    randomize: bool = False, \n",
    "    limit: Optional[int] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if randomize:\n",
    "        random.shuffle(data)\n",
    "\n",
    "    if limit is not None:\n",
    "        data = data[:limit]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7224d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(a: str, b: str) -> float:\n",
    "    a_tokens = set(a.lower().split())\n",
    "    b_tokens = set(b.lower().split())\n",
    "\n",
    "    if not a_tokens or not b_tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = a_tokens.intersection(b_tokens)\n",
    "    union = a_tokens.union(b_tokens)\n",
    "    \n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e18c996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant(ground_truth: str, doc_content: str, threshold: float = 50, jaccard_threshold: float = 0.3) -> bool:\n",
    "    ground_truth = ground_truth.lower().strip()\n",
    "    doc_content = doc_content.lower().strip()\n",
    "\n",
    "    # Exact substring match\n",
    "    if ground_truth in doc_content or doc_content in ground_truth:\n",
    "        return True\n",
    "\n",
    "    # Bi-directional fuzzy match\n",
    "    similarity_1 = fuzz.partial_ratio(ground_truth, doc_content)\n",
    "    similarity_2 = fuzz.partial_ratio(doc_content, ground_truth)\n",
    "    if max(similarity_1, similarity_2) >= threshold:\n",
    "        return True\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    jaccard = jaccard_similarity(ground_truth, doc_content)\n",
    "    if jaccard >= jaccard_threshold:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfea1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query_with_gemma(question: str) -> dict:\n",
    "    prompt = (\n",
    "        f\"Given the following question:\\n\\n\\\"{question}\\\"\\n\\n\"\n",
    "        \"Classify this question with:\\n\"\n",
    "        \"Category: <a broad category in one word>\\n\"\n",
    "        \"Tags: <comma-separated 2 to 3 relevant keywords>\"\n",
    "    )\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(ollama_url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        output = response.json()[\"response\"]\n",
    "        category, tags = \"unknown\", []\n",
    "        for line in output.strip().split(\"\\n\"):\n",
    "            if line.lower().startswith(\"category:\"):\n",
    "                category = line.split(\":\", 1)[1].strip().strip(\"*\")\n",
    "            elif line.lower().startswith(\"tags:\"):\n",
    "                tags = [t.strip().strip(\"*\") for t in line.split(\":\", 1)[1].split(\",\")]\n",
    "        return {\"category\": category, \"tags\": tags}\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to classify: {question}\")\n",
    "        return {\"category\": \"unknown\", \"tags\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b9c0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_by_metadata(query: str,\n",
    "                         category: str,\n",
    "                         tags: List[str],\n",
    "                         top_k: int = 3,\n",
    "                         verbose = False,\n",
    "                         category_threshold: float = 0.6,\n",
    "                         fuzzy_tag_threshold: int = 80) -> List[Any]:\n",
    "\n",
    "    category_embedding = embedding_model.embed_query(category)\n",
    "    candidate_docs = vectorstore.similarity_search(query=query, k=50)\n",
    "\n",
    "    filtered_docs = []\n",
    "    seen = set()\n",
    "\n",
    "    category_pass = 0\n",
    "    tag_pass = 0\n",
    "    total_skipped_no_meta = 0\n",
    "    total_seen = 0\n",
    "\n",
    "    for doc in candidate_docs:\n",
    "        total_seen += 1\n",
    "        doc_meta = doc.metadata or {}\n",
    "\n",
    "        # --- Category similarity check ---\n",
    "        doc_cat = doc_meta.get(\"category\", \"\").strip()\n",
    "        if not doc_cat:\n",
    "            total_skipped_no_meta += 1\n",
    "            continue\n",
    "\n",
    "        doc_cat_embedding = embedding_model.embed_query(doc_cat)\n",
    "        cat_sim = cosine_similarity([category_embedding], [doc_cat_embedding])[0][0]\n",
    "\n",
    "        if cat_sim < category_threshold:\n",
    "            continue\n",
    "        category_pass += 1\n",
    "\n",
    "        # --- Tag fuzzy matching (allow partial matches) ---\n",
    "        doc_tags = doc_meta.get(\"tags\", \"\")\n",
    "        if isinstance(doc_tags, str):\n",
    "            doc_tags = [t.strip() for t in doc_tags.split(\",\") if t.strip()]\n",
    "        if not doc_tags:\n",
    "            continue\n",
    "\n",
    "        tag_scores = [fuzz.partial_ratio(tag.lower(), dt.lower())\n",
    "                      for tag in tags for dt in doc_tags]\n",
    "        if not tag_scores:\n",
    "            continue\n",
    "\n",
    "        max_tag_score = max(tag_scores)\n",
    "        avg_top3_score = mean(sorted(tag_scores, reverse=True)[:3])\n",
    "\n",
    "        if max_tag_score < fuzzy_tag_threshold and avg_top3_score < (fuzzy_tag_threshold - 10):\n",
    "            continue\n",
    "        tag_pass += 1\n",
    "\n",
    "        # Deduplicate by content\n",
    "        snippet = doc.page_content[:50].strip().lower()\n",
    "        if snippet in seen:\n",
    "            continue\n",
    "\n",
    "        seen.add(snippet)\n",
    "        filtered_docs.append(doc)\n",
    "        if len(filtered_docs) >= top_k:\n",
    "            break\n",
    "\n",
    "    # --- Logging ---\n",
    "    if verbose:\n",
    "        print(f\"\\n[INFO] Retrieved: {len(candidate_docs)} Top: candidates for query: \\\"{query[:128]}...\\\"\")\n",
    "        print(f\"[INFO] Skipped (missing metadata): {total_skipped_no_meta}\")\n",
    "        print(f\"[INFO] Passed category filter: {category_pass}\")\n",
    "        print(f\"[INFO] Passed tag filter: {tag_pass}\")\n",
    "        print(f\"[INFO] Final top-k after deduplication: {len(filtered_docs)}\\n\")\n",
    "\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d1d0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(\n",
    "    eval_data: list,\n",
    "    reranker,\n",
    "    k=3,\n",
    "    fuzzy_threshold=70,\n",
    "    jaccard_threshold=0.4,\n",
    "    doc_content_key=\"page_content\"\n",
    "):\n",
    "    import sys\n",
    "\n",
    "    hits = 0\n",
    "    relevant_docs_total = 0\n",
    "    retrieved_docs_total = 0\n",
    "    reciprocal_ranks = []\n",
    "    results_flat = []\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    csv_path = f\"results/result_{timestamp}.csv\"\n",
    "\n",
    "    tqdm_params = dict(\n",
    "        desc=\"Evaluating\",\n",
    "        dynamic_ncols=True,\n",
    "        file=sys.stdout,\n",
    "        leave=True,\n",
    "        mininterval=900.0   # 15 minutes\n",
    "    )\n",
    "\n",
    "    for idx, sample in enumerate(tqdm(eval_data, **tqdm_params)):\n",
    "        question = sample[\"question\"]\n",
    "        ground_truth = sample[\"answer\"]\n",
    "\n",
    "        classification = classify_query_with_gemma(question)\n",
    "        category = classification[\"category\"]\n",
    "        tags = classification[\"tags\"]\n",
    "\n",
    "        if not category or category.lower() == \"unknown\":\n",
    "            print(f\"[WARN] Question {idx+1} got weak category from Gemma: {question[:60]}...\")\n",
    "\n",
    "        try:\n",
    "            initial_docs = retrieve_by_metadata(question, category, tags, top_k=k, verbose=False)\n",
    "            if not initial_docs:\n",
    "                reciprocal_ranks.append(0)\n",
    "                results_flat.append({\n",
    "                    \"item\": idx + 1,\n",
    "                    \"query\": question,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                    \"category\": category,\n",
    "                    \"tags\": \", \".join(tags),\n",
    "                    \"rank\": 1,\n",
    "                    \"score\": \"N/A\",\n",
    "                    \"found\": False,\n",
    "                    \"doc_content\": \"[NO DOCUMENTS RETRIEVED]\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            reranked = reranker(question, initial_docs, top_n=k)\n",
    "            if not reranked:\n",
    "                reciprocal_ranks.append(0)\n",
    "                results_flat.append({\n",
    "                    \"item\": idx + 1,\n",
    "                    \"query\": question,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                    \"category\": category,\n",
    "                    \"tags\": \", \".join(tags),\n",
    "                    \"rank\": 1,\n",
    "                    \"score\": \"N/A\",\n",
    "                    \"found\": False,\n",
    "                    \"doc_content\": \"[NO DOCUMENTS RERANKED]\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            found = False\n",
    "            retrieved_docs_total += len(reranked)\n",
    "\n",
    "            for rank, (score, doc) in enumerate(reranked):\n",
    "                doc_content = getattr(doc, doc_content_key, doc)\n",
    "                if isinstance(doc_content, dict):\n",
    "                    doc_content = doc_content.get(\"content\", \"\")\n",
    "\n",
    "                if is_relevant(ground_truth, doc_content, fuzzy_threshold, jaccard_threshold):\n",
    "                    relevant_docs_total += 1\n",
    "                    if not found:\n",
    "                        hits += 1\n",
    "                        reciprocal_ranks.append(1 / (rank + 1))\n",
    "                        found = True\n",
    "\n",
    "                results_flat.append({\n",
    "                    \"item\": idx + 1,\n",
    "                    \"query\": question,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                    \"category\": category,\n",
    "                    \"tags\": \", \".join(tags),\n",
    "                    \"rank\": rank + 1,\n",
    "                    \"score\": f\"{score:.4f}\",\n",
    "                    \"found\": found,\n",
    "                    \"doc_content\": doc_content[:500]\n",
    "                })\n",
    "\n",
    "            # If nothing relevant was found, log rank 1 again with found=False\n",
    "            if not found:\n",
    "                reciprocal_ranks.append(0)\n",
    "                if reranked:\n",
    "                    score, doc = reranked[0]\n",
    "                    doc_content = getattr(doc, doc_content_key, doc)\n",
    "                    if isinstance(doc_content, dict):\n",
    "                        doc_content = doc_content.get(\"content\", \"\")\n",
    "\n",
    "                    results_flat.append({\n",
    "                        \"item\": idx + 1,\n",
    "                        \"query\": question,\n",
    "                        \"ground_truth\": ground_truth,\n",
    "                        \"category\": category,\n",
    "                        \"tags\": \", \".join(tags),\n",
    "                        \"rank\": 1,\n",
    "                        \"score\": f\"{score:.4f}\",\n",
    "                        \"found\": False,\n",
    "                        \"doc_content\": doc_content[:500]\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            reciprocal_ranks.append(0)\n",
    "            results_flat.append({\n",
    "                \"item\": idx + 1,\n",
    "                \"query\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"category\": category,\n",
    "                \"tags\": \", \".join(tags),\n",
    "                \"rank\": \"error\",\n",
    "                \"score\": \"N/A\",\n",
    "                \"found\": False,\n",
    "                \"doc_content\": f\"[ERROR] {str(e)}\"\n",
    "            })\n",
    "\n",
    "    # Save output\n",
    "    df = pd.DataFrame(results_flat)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\n[INFO] Saved CSV results to: {csv_path}\")\n",
    "\n",
    "    total_queries = len(eval_data)\n",
    "    metrics = {\n",
    "        f\"Recall@{k}\": hits / total_queries if total_queries > 0 else 0.0,\n",
    "        f\"Precision@{k}\": relevant_docs_total / retrieved_docs_total if retrieved_docs_total > 0 else 0.0,\n",
    "        f\"MRR@{k}\": np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "    }\n",
    "\n",
    "    return metrics, csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4388300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, docs, top_n=3, verbose=False):\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    normalized_scores = [score / (len(doc.page_content.split()) + 1) for score, doc in zip(scores, docs)]\n",
    "    scored_docs = list(zip(normalized_scores, docs))\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        for i, (score, doc) in enumerate(scored_docs[:top_n], start=1):\n",
    "            print(f\"\\nRank {i} Score: {score:.4f}\")\n",
    "            print(doc.page_content[:300] + \"...\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    return scored_docs[:top_n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "be369b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3/3 [00:33<00:00, 11.06s/it]\n",
      "\n",
      "[INFO] Saved CSV results to: results/result_06-27-2025_20-51-44.csv\n"
     ]
    }
   ],
   "source": [
    "eval_data = load_eval_data(\"evaluation.json\", randomize=False, limit=3)\n",
    "\n",
    "results, csv_path = evaluate_retriever(\n",
    "    eval_data=eval_data,\n",
    "    reranker=rerank_with_cross_encoder,\n",
    "    k=3,\n",
    "    fuzzy_threshold=70,\n",
    "    jaccard_threshold = 0.4,\n",
    "    doc_content_key='page_content'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ce91eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Recall@3: 0.3333\n",
      "Precision@3: 0.1667\n",
      "MRR@3: 0.3333\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99b419",
   "metadata": {},
   "source": [
    "# View Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32adec29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>found</th>\n",
       "      <th>doc_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Who is the national hero of the Philippines?</td>\n",
       "      <td>Jose Rizal</td>\n",
       "      <td>History</td>\n",
       "      <td>Philippines, Hero, Nationalism</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>True</td>\n",
       "      <td>. The nineteenth-century revolutionary General Artemio Ricarte once proposed naming the country the Rizaline Islands, after its foremost national hero, Jos Rizal (with Filipinos henceforth to be known as Rizalinos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Who is the national hero of the Philippines?</td>\n",
       "      <td>Jose Rizal</td>\n",
       "      <td>History</td>\n",
       "      <td>Philippines, Hero, Nationalism</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>True</td>\n",
       "      <td>. Aguinaldo declared the independence of the Philippines on June 12 Commodore Dewey was invited to the momentous occasion but he begged off, saying it was mail day and for the first time, the Philippine flag was displayed and the national anthem sung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Who is the national hero of the Philippines?</td>\n",
       "      <td>Jose Rizal</td>\n",
       "      <td>History</td>\n",
       "      <td>Philippines, Hero, Nationalism</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.1318</td>\n",
       "      <td>True</td>\n",
       "      <td>. Supportive of the Japanese, he was a living symbol of the Filipino struggle for independence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Ano ang ibig sabihin ng ‘bayanihan’ sa kulturang Pilipino?</td>\n",
       "      <td>Ang bayanihan ay ang tradisyon ng pagtutulungan at pagkakaisa sa komunidad, tulad ng sama-samang paglilipat ng bahay-kubo.</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Filipino, Tradition, Community</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.2663</td>\n",
       "      <td>False</td>\n",
       "      <td>. Filipino immigrant communities formed mostly bachelor societies women were discouraged from emigrating, primarily to forestall the formation of families and the putting down of roots and would essentially remain so until the end of World War II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Ano ang ibig sabihin ng ‘bayanihan’ sa kulturang Pilipino?</td>\n",
       "      <td>Ang bayanihan ay ang tradisyon ng pagtutulungan at pagkakaisa sa komunidad, tulad ng sama-samang paglilipat ng bahay-kubo.</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Filipino, Tradition, Community</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.2680</td>\n",
       "      <td>False</td>\n",
       "      <td>. In this in-between state, they were defined as Philippine citizens, under the protection of the U.S., free to enter the country and look for work. On the other hand, they couldn t vote, own land, or marry white women</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item                                                       query                                                                                                                ground_truth category                            tags  rank   score  found                                                                                                                                                                                                                                                 doc_content\n",
       "0     1                Who is the national hero of the Philippines?                                                                                                                  Jose Rizal  History  Philippines, Hero, Nationalism     1  0.1760   True                                      . The nineteenth-century revolutionary General Artemio Ricarte once proposed naming the country the Rizaline Islands, after its foremost national hero, Jos Rizal (with Filipinos henceforth to be known as Rizalinos)\n",
       "1     1                Who is the national hero of the Philippines?                                                                                                                  Jose Rizal  History  Philippines, Hero, Nationalism     2  0.0311   True  . Aguinaldo declared the independence of the Philippines on June 12 Commodore Dewey was invited to the momentous occasion but he begged off, saying it was mail day and for the first time, the Philippine flag was displayed and the national anthem sung\n",
       "2     1                Who is the national hero of the Philippines?                                                                                                                  Jose Rizal  History  Philippines, Hero, Nationalism     3 -0.1318   True                                                                                                                                                              . Supportive of the Japanese, he was a living symbol of the Filipino struggle for independence\n",
       "3     2  Ano ang ibig sabihin ng ‘bayanihan’ sa kulturang Pilipino?  Ang bayanihan ay ang tradisyon ng pagtutulungan at pagkakaisa sa komunidad, tulad ng sama-samang paglilipat ng bahay-kubo.  Culture  Filipino, Tradition, Community     1 -0.2663  False      . Filipino immigrant communities formed mostly bachelor societies women were discouraged from emigrating, primarily to forestall the formation of families and the putting down of roots and would essentially remain so until the end of World War II\n",
       "4     2  Ano ang ibig sabihin ng ‘bayanihan’ sa kulturang Pilipino?  Ang bayanihan ay ang tradisyon ng pagtutulungan at pagkakaisa sa komunidad, tulad ng sama-samang paglilipat ng bahay-kubo.  Culture  Filipino, Tradition, Community     2 -0.2680  False                                  . In this in-between state, they were defined as Philippine citizens, under the protection of the U.S., free to enter the country and look for work. On the other hand, they couldn t vote, own land, or marry white women"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837418c",
   "metadata": {},
   "source": [
    "# SINGLE QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "427e5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Output\n",
    "query = \"What is the most famous Filipino dish?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0b461f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Category: Food\n",
      "[INFO] Tags: ['Filipino cuisine', 'Adobo', 'Adobo recipes']\n"
     ]
    }
   ],
   "source": [
    "# Classify with Gemma\n",
    "classification = classify_query_with_gemma(query)\n",
    "category = classification[\"category\"]\n",
    "tags = classification[\"tags\"]\n",
    "\n",
    "print(f\"[INFO] Category: {category}\")\n",
    "print(f\"[INFO] Tags: {tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a79f18f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Retrieved: 50 Top: candidates for query: \"What is the most famous Filipino dish?...\"\n",
      "[INFO] Skipped (missing metadata): 0\n",
      "[INFO] Passed category filter: 2\n",
      "[INFO] Passed tag filter: 0\n",
      "[INFO] Final top-k after deduplication: 0\n",
      "\n",
      "[WARN] No documents found after filtering.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve documents by category and tags\n",
    "initial_docs = retrieve_by_metadata(query, category, tags, top_k=10, verbose=True)\n",
    "\n",
    "if not initial_docs:\n",
    "    print(\"[WARN] No documents found after filtering.\")\n",
    "else:\n",
    "    reranked = rerank_with_cross_encoder(query, initial_docs, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ba5b66a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 | Score: -0.1060\n",
      "------------------------------------------------------------\n",
      ". Unable to attend to their fields, the huddled population suffered an inordinate rate of death due to malnutrition and disease, such as dengue fever and malaria. In Batangas alone, 54,000 civilians died by the end of 1901\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 2 | Score: -0.1207\n",
      "------------------------------------------------------------\n",
      ". Thence, Islam spread to Maranao territory (around Lake Danao), inexplicably skipped the Visayas for the most part, and sailed north to Mindoro Island and Maynila and Batangas in Luzon, and west to southern Palawan\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 3 | Score: -0.2222\n",
      "------------------------------------------------------------\n",
      ".) The very nature of the archipelago, with more water than land, meant the existence of numerous tribal groups, other than the dominant ones the Tagalogs, the Kapampangans, the Ilocanos, the Bicolanos, the Cebuanos, and other Visayans\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print output\n",
    "for i, (score, doc) in enumerate(reranked, start=1):\n",
    "    print(f\"\\nRank {i} | Score: {score:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(doc.page_content.strip()[:300])\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
