{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb57e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import wikipedia\n",
    "import torch\n",
    "import wikipediaapi\n",
    "import time\n",
    "import fitz\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1abd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"history-of-the-filipino-people.pdf\"\n",
    "json_path = \"knowledge_base/filipino_history_pdf.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a51619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping PDF extraction – file already exists: knowledge_base/filipino_history_pdf.json\n"
     ]
    }
   ],
   "source": [
    "# Scrapping History of the Filipino People PDF\n",
    "if os.path.exists(json_path):\n",
    "    print(f\"[INFO] Skipping PDF extraction – file already exists: {json_path}\")\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    print(f\"[INFO] Extracting text from: {pdf_path}\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    data = {}\n",
    "\n",
    "    # Extract text from each page\n",
    "    for page_number in tqdm(range(len(doc)), desc=\"Extracting PDF\"):\n",
    "        page = doc[page_number]\n",
    "        text = page.get_text().strip()\n",
    "\n",
    "        if text:\n",
    "            data[f\"page_{page_number + 1}\"] = {\n",
    "                \"page\": page_number + 1,\n",
    "                \"content\": text\n",
    "            }\n",
    "\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[INFO] Saved PDF content to {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5bacfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wikipedia API for English and Tagalog\n",
    "wiki_en = wikipediaapi.Wikipedia(language='en', user_agent='WiQAS/1.0 (ralf_hernandez@dlsu.edu.ph)')\n",
    "wiki_tl = wikipediaapi.Wikipedia(language='tl', user_agent='WiQAS/1.0 (ralf_hernandez@dlsu.edu.ph)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb55d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all pages in a category recursively\n",
    "def scrape_category(category, wiki, scraped_pages=None, depth=0, max_depth=3):\n",
    "    if scraped_pages is None:\n",
    "        scraped_pages = {}\n",
    "    \n",
    "    # Avoid infinite recursion\n",
    "    if depth > max_depth:\n",
    "        return scraped_pages\n",
    "    \n",
    "    # Get category members\n",
    "    cat = wiki.page(f\"Category:{category}\")\n",
    "    if not cat.exists():\n",
    "        return scraped_pages\n",
    "    \n",
    "    # Iterate through category members\n",
    "    for member_name, member_page in tqdm(cat.categorymembers.items(), desc=f\"Scraping {category}\"):\n",
    "        if member_page.namespace == wikipediaapi.Namespace.CATEGORY:\n",
    "            # Recursively scrape subcategories\n",
    "            print(f\"[INFO] Processing: {member_name} | Depth: {depth}\")\n",
    "            scrape_category(member_name.replace(\"Category:\", \"\"), wiki, scraped_pages, depth + 1, max_depth)\n",
    "        else:\n",
    "            # Process articles\n",
    "            try:\n",
    "                if member_page.exists() and member_name not in scraped_pages:\n",
    "                    scraped_pages[member_name] = {\n",
    "                        'title': member_page.title,\n",
    "                        'content': member_page.text,\n",
    "                        'url': member_page.fullurl\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {member_name}: {e}\")\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    return scraped_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English Wikipedia scraping\n",
    "english_json_path = \"knowledge_base/philippine_culture_en.json\"\n",
    "\n",
    "if os.path.exists(english_json_path):\n",
    "    print(f\"[INFO] Skipping English scrape – file already exists: {english_json_path}\")\n",
    "    with open(english_json_path, 'r', encoding='utf-8') as f:\n",
    "        english_data = json.load(f)\n",
    "else:\n",
    "    print(\"\\n[INFO] Scraping English category: Culture_of_the_Philippines\")\n",
    "    english_data = scrape_category('Culture_of_the_Philippines', wiki_en)\n",
    "\n",
    "    with open(english_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(english_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[INFO] Saved to {english_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f630c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping Tagalog scrape – file already exists: knowledge_base/philippine_culture_tl.json\n"
     ]
    }
   ],
   "source": [
    "# Tagalog Wikipedia scraping\n",
    "tagalog_json_path = \"knowledge_base/philippine_culture_tl.json\"\n",
    "\n",
    "if os.path.exists(tagalog_json_path):\n",
    "    print(f\"[INFO] Skipping Tagalog scrape – file already exists: {tagalog_json_path}\")\n",
    "    with open(tagalog_json_path, 'r', encoding='utf-8') as f:\n",
    "        tagalog_data = json.load(f)\n",
    "else:\n",
    "    print(\"\\n[INFO] Scraping Tagalog category: Kultura_ng_Pilipinas\")\n",
    "    tagalog_data = scrape_category('Kultura_ng_Pilipinas', wiki_tl)\n",
    "\n",
    "    with open(tagalog_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tagalog_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[INFO] Saved to {tagalog_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56611181",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_dir = \"knowledge_base\"\n",
    "processed_dir = \"knowledge_base/cleaned\"\n",
    "os.makedirs(processed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "959709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def clean_text(text):\n",
    "    # Basic cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = re.sub(r'\\.{3,}', '.', text)  # Collapse ellipsis\n",
    "    text = re.sub(r'\\s+\\.', '.', text)  # Remove space before periods\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove common noise headers (language-dependent)\n",
    "    for noisy_section in [\"See also\", \"References\", \"Mga sanggunian\", \"Tingnan din\"]:\n",
    "        if noisy_section.lower() in text.lower():\n",
    "            text = text[:text.lower().find(noisy_section.lower())]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing filipino_history_pdf.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning filipino_history_pdf.json: 100%|██████████| 645/645 [00:00<00:00, 4182.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleaned file saved to: knowledge_base/cleaned\\filipino_history_pdf.json\n",
      "[INFO] Processing philippine_culture_tl.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning philippine_culture_tl.json: 100%|██████████| 93/93 [00:00<00:00, 2364.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleaned file saved to: knowledge_base/cleaned\\philippine_culture_tl.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all JSON files\n",
    "for filename in os.listdir(kb_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        input_path = os.path.join(kb_dir, filename)\n",
    "        output_path = os.path.join(processed_dir, filename)\n",
    "\n",
    "        print(f\"[INFO] Processing {filename}\")\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        cleaned_data = {}\n",
    "\n",
    "        for key, value in tqdm(raw_data.items(), desc=f\"Cleaning {filename}\"):\n",
    "            text = value.get(\"content\") or value.get(\"text\") or \"\"\n",
    "            cleaned = clean_text(text)\n",
    "            if len(cleaned.split()) >= 5:\n",
    "                cleaned_data[key] = {\n",
    "                    **value,\n",
    "                    \"cleaned_content\": cleaned\n",
    "                }\n",
    "\n",
    "        # Save cleaned output\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Cleaned file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3f96891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ralfh\\AppData\\Local\\Temp\\ipykernel_5984\\826302932.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Dense Embeddings (BGE-M3)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e892a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74312637",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for doc in cleaned_data.values():\n",
    "    text = doc[\"cleaned_content\"]\n",
    "    split_chunks = splitter.split_text(text)\n",
    "    chunks.extend(split_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f62001b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma Vector Store (Dense Retrieval)\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"filipino_culture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23fd0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks)\n",
    "bm25_retriever.k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        bm25_retriever\n",
    "    ],\n",
    "    weights=[0.85, 0.15]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1dc38cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, docs, top_n=3, verbose=False):\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    scored_docs = list(zip(scores, docs))\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        for i, (score, doc) in enumerate(scored_docs[:top_n], start=1):\n",
    "            print(f\"\\nRank {i} Score: {score:.4f}\")\n",
    "            print(doc.page_content[:350] + \"...\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    return scored_docs[:top_n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a975b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 Score: 6.5966\n",
      "Itinuturing ang kanyang pinakagrandeng akda, ang Florante at Laura bilang isa sa mga obra-maestra ng Panitikang Filipino. Isinulat ni Balagtas ang epiko noong nasa bilangguan siya. Si José Rizal, ang pambansang bayani, ay nagsulat ng mga nobelang Noli Me Tángere (Huwag Akong Salangin Nino Man) at El Filibusterismo (Ang Pilibusterismo, kilala rin bi...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 2 Score: 5.9323\n",
      ". Nagsulat si Jose Rizal ng kanyang mga librong Noli Me Tangere at El filibusterismo kahit alam niyang na maaari siyang ipapatay ng mga Kastila kung makita nila ito. Dahil sa matapang na aksyon ni Dr. Rizal, nagising ang mga Filipino sa kaapihan na naranasan nila sa ilalim ng mga Kastila, at sinimulan nila ang mga rebolusyon upang makamit ang kalay...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 3 Score: 5.4439\n",
      ". Upang kumita ng salapi, sumali si Amorsolo sa mga paligsahan at gumawa rin siya ng mga guhit para sa mga palimbagan sa Pilipinas, kabilang ang isang larawan para sa pabalat ng pinakunang nobelang isinulat ni Severino Reyes sa wikang Tagalog, na ang Parusa ng Diyos. Gumuhit din siya ng larawan para sa Madaling Araw ni Iñigo Ed. Regalado, at para s...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Output\n",
    "query = \"Ano ang nobelang isinulat ni Jose Rizal na tumatalakay sa mga Pilipino at kanilang kalagayan sa ilalim ng kolonyal na pamahalaan?\"\n",
    "# Retrieve documents using hybrid retriever\n",
    "initial_results = ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "# Rerank using Cross-Encoder\n",
    "reranked = rerank_with_cross_encoder(query, initial_results, top_n=3, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
