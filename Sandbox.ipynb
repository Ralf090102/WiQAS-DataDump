{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb57e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import wikipedia\n",
    "import torch\n",
    "import wikipediaapi\n",
    "import time\n",
    "import fitz\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d174d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_section_title(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if re.match(r'^(Chapter \\d+|[\\dI]+\\.\\s+[A-Za-z\\s]+)', line, re.IGNORECASE):\n",
    "            return line\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1abd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"history-of-the-filipino-people.pdf\"\n",
    "json_path = \"knowledge_base/filipino_history_pdf.json\"\n",
    "directory = os.path.dirname(json_path)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"[INFO] Created directory: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping PDF extraction – file already exists: knowledge_base/filipino_history_pdf.json\n"
     ]
    }
   ],
   "source": [
    "# Scrapping History of the Filipino People PDF\n",
    "if os.path.exists(json_path):\n",
    "    print(f\"[INFO] Skipping PDF extraction – file already exists: {json_path}\")\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    print(f\"[INFO] Extracting text from: {pdf_path}\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    data = {}\n",
    "\n",
    "    # Extract text from each page\n",
    "    for page_number in tqdm(range(len(doc)), desc=\"Extracting PDF\"):\n",
    "        page = doc[page_number]\n",
    "        text = page.get_text().strip()\n",
    "\n",
    "        if text:\n",
    "            # Detect section title (optional)\n",
    "            section = detect_section_title(text)\n",
    "            \n",
    "            data[f\"page_{page_number + 1}\"] = {\n",
    "                \"page\": page_number + 1,\n",
    "                \"content\": text,\n",
    "                \"metadata\": {\n",
    "                    \"source\": \"Agoncillo_textbook\",\n",
    "                    \"document_type\": \"textbook\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"section\": section\n",
    "                }\n",
    "            }\n",
    "\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[INFO] Saved PDF content to {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5bacfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wikipedia API for English and Tagalog\n",
    "wiki_en = wikipediaapi.Wikipedia(language='en', user_agent='WiQAS/1.0 (ralf_hernandez@dlsu.edu.ph)')\n",
    "wiki_tl = wikipediaapi.Wikipedia(language='tl', user_agent='WiQAS/1.0 (ralf_hernandez@dlsu.edu.ph)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb55d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all pages in a category recursively\n",
    "def scrape_category(category, wiki, language, scraped_pages=None, depth=0, max_depth=3):\n",
    "    if scraped_pages is None:\n",
    "        scraped_pages = {}\n",
    "    \n",
    "    # Avoid infinite recursion\n",
    "    if depth > max_depth:\n",
    "        return scraped_pages\n",
    "    \n",
    "    # Get category members\n",
    "    cat = wiki.page(f\"Category:{category}\")\n",
    "    if not cat.exists():\n",
    "        return scraped_pages\n",
    "    \n",
    "    # Iterate through category members\n",
    "    for member_name, member_page in tqdm(cat.categorymembers.items(), desc=f\"Scraping {category}\"):\n",
    "        if member_page.namespace == wikipediaapi.Namespace.CATEGORY:\n",
    "            print(f\"[INFO] Processing: {member_name} | Depth: {depth}\")\n",
    "            scrape_category(member_name.replace(\"Category:\", \"\"), wiki, language, scraped_pages, depth + 1, max_depth)\n",
    "        else:\n",
    "            try:\n",
    "                if member_page.exists() and member_name not in scraped_pages:\n",
    "                    scraped_pages[member_name] = {\n",
    "                        \"title\": member_page.title,\n",
    "                        \"content\": member_page.text,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": f\"Wikipedia_{language}\",\n",
    "                            \"language\": \"English\" if language == \"en\" else \"Tagalog\",\n",
    "                            \"url\": member_page.fullurl,\n",
    "                            \"category\": category,\n",
    "                            \"namespace\": \"Main\"\n",
    "                        }\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {member_name}: {e}\")\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    return scraped_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa5915",
   "metadata": {},
   "source": [
    "# TAKES TOO LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd2d8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English Wikipedia scraping\n",
    "# english_json_path = \"knowledge_base/philippine_culture_en.json\"\n",
    "\n",
    "# if os.path.exists(english_json_path):\n",
    "#     print(f\"[INFO] Skipping English scrape – file already exists: {english_json_path}\")\n",
    "#     with open(english_json_path, 'r', encoding='utf-8') as f:\n",
    "#         english_data = json.load(f)\n",
    "# else:\n",
    "#     print(\"\\n[INFO] Scraping English category: Culture_of_the_Philippines\")\n",
    "#     english_data = scrape_category('Culture_of_the_Philippines', wiki_en, language=\"en\")\n",
    "\n",
    "#     with open(english_json_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(english_data, f, ensure_ascii=False, indent=2)\n",
    "#     print(f\"[INFO] Saved to {english_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f630c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping Tagalog scrape – file already exists: knowledge_base/philippine_culture_tl.json\n"
     ]
    }
   ],
   "source": [
    "# Tagalog Wikipedia scraping\n",
    "tagalog_json_path = \"knowledge_base/philippine_culture_tl.json\"\n",
    "\n",
    "if os.path.exists(tagalog_json_path):\n",
    "    print(f\"[INFO] Skipping Tagalog scrape – file already exists: {tagalog_json_path}\")\n",
    "    with open(tagalog_json_path, 'r', encoding='utf-8') as f:\n",
    "        tagalog_data = json.load(f)\n",
    "else:\n",
    "    print(\"\\n[INFO] Scraping Tagalog category: Kultura_ng_Pilipinas\")\n",
    "    tagalog_data = scrape_category('Kultura_ng_Pilipinas', wiki_tl, language=\"tl\")\n",
    "\n",
    "    with open(tagalog_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tagalog_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[INFO] Saved to {tagalog_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56611181",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_dir = \"./knowledge_base\"\n",
    "processed_dir = \"./processed_data\"\n",
    "os.makedirs(processed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "959709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def clean_text(text):\n",
    "    # Normalize whitespace and punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\.{3,}', '.', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    # Remove Wikipedia-specific noise\n",
    "    text = re.sub(r'\\{\\{.*?\\}\\}', '', text)  # Remove templates\n",
    "    text = re.sub(r'\\[\\[.*?\\]\\]', '', text)  # Remove internal links\n",
    "    text = re.sub(r'\\[edit\\]', '', text)  # Remove edit tags\n",
    "    text = re.sub(r'Category:.*|Kategorya:.*', '', text)  # Remove category tags\n",
    "    \n",
    "    # Remove common Wikipedia and textbook sections\n",
    "    noisy_sections = [\n",
    "        \"See also\", \"References\", \"External links\", \"Mga sanggunian\", \n",
    "        \"Tingnan din\", \"Mga kawing panlabas\", \"Table of Contents\", \n",
    "        \"Preface\", \"Foreword\", \"Appendix\", \"Talaan ng mga nilalaman\"\n",
    "    ]\n",
    "    for section in noisy_sections:\n",
    "        if section.lower() in text.lower():\n",
    "            text = text[:text.lower().find(section.lower())]\n",
    "    \n",
    "    # Remove short or low-value text\n",
    "    text = text.strip()\n",
    "    return text if len(text.split()) >= 5 else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a088ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing filipino_history_pdf.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning filipino_history_pdf.json: 100%|██████████| 645/645 [00:00<00:00, 3489.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleaned file saved to: ./processed_data\\filipino_history_pdf.json\n",
      "[INFO] Processing philippine_culture_tl.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning philippine_culture_tl.json: 100%|██████████| 93/93 [00:00<00:00, 1921.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleaned file saved to: ./processed_data\\philippine_culture_tl.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all JSON files\n",
    "for filename in os.listdir(kb_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        input_path = os.path.join(kb_dir, filename)\n",
    "        output_path = os.path.join(processed_dir, filename)\n",
    "\n",
    "        print(f\"[INFO] Processing {filename}\")\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        cleaned_data = {}\n",
    "\n",
    "        for key, value in tqdm(raw_data.items(), desc=f\"Cleaning {filename}\"):\n",
    "            text = value.get(\"content\") or value.get(\"text\") or \"\"\n",
    "            cleaned = clean_text(text)\n",
    "            if len(cleaned.split()) >= 5:\n",
    "                metadata = value.get(\"metadata\", {})\n",
    "                if \"source\" not in metadata:\n",
    "                    if \"Agoncillo\" in filename:\n",
    "                        metadata[\"source\"] = \"Agoncillo_textbook\"\n",
    "                    elif \"en\" in filename:\n",
    "                        metadata[\"source\"] = \"Wikipedia_en\"\n",
    "                    elif \"tl\" in filename:\n",
    "                        metadata[\"source\"] = \"Wikipedia_tl\"\n",
    "                    else:\n",
    "                        metadata[\"source\"] = \"unknown\"\n",
    "                metadata[\"source_file\"] = filename\n",
    "                cleaned_data[key] = {\n",
    "                    **value,\n",
    "                    \"cleaned_content\": cleaned,\n",
    "                    \"metadata\": metadata\n",
    "                }\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Cleaned file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f96891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herna\\AppData\\Local\\Temp\\ipykernel_12784\\826302932.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Dense Embeddings (BGE-M3)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4722d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate Chunks\n",
    "def deduplicate_chunks(chunks_with_metadata, threshold=0.95):\n",
    "    texts = [c[\"text\"] for c in chunks_with_metadata]\n",
    "    vectorizer = TfidfVectorizer().fit_transform(texts)\n",
    "    similarities = cosine_similarity(vectorizer)\n",
    "    unique_chunks = []\n",
    "    seen = set()\n",
    "    for i, chunk in enumerate(chunks_with_metadata):\n",
    "        if i not in seen:\n",
    "            unique_chunks.append(chunk)\n",
    "            for j in range(i + 1, len(texts)):\n",
    "                if similarities[i, j] > threshold:\n",
    "                    seen.add(j)\n",
    "    return unique_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e892a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=75,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74312637",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_with_metadata = []\n",
    "for key, doc in cleaned_data.items():\n",
    "    text = doc[\"cleaned_content\"]\n",
    "    # Access metadata safely\n",
    "    metadata = doc.get(\"metadata\", {})\n",
    "    source = metadata.get(\"source\", \"unknown\")\n",
    "    section = metadata.get(\"section\", \"unknown\")\n",
    "    split_chunks = splitter.split_text(text)\n",
    "    chunks_with_metadata.extend([\n",
    "        {\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"source\": source,\n",
    "                \"section\": section,\n",
    "                \"source_file\": metadata.get(\"source_file\", \"unknown\"),\n",
    "                \"chunk_id\": f\"{key}_{i}\"\n",
    "            }\n",
    "        }\n",
    "        for i, chunk in enumerate(split_chunks)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26d12713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate\n",
    "chunks_with_metadata = deduplicate_chunks(chunks_with_metadata)\n",
    "chunks = [c[\"text\"] for c in chunks_with_metadata]\n",
    "metadata = [c[\"metadata\"] for c in chunks_with_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f62001b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma Vector Store (Dense Retrieval)\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    metadatas=metadata,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"filipino_culture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23fd0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks)\n",
    "bm25_retriever.k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ea1b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        bm25_retriever\n",
    "    ],\n",
    "    weights=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dc38cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, docs, top_n=3, verbose=False):\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    normalized_scores = [score / (len(doc.page_content.split()) + 1) for score, doc in zip(scores, docs)]\n",
    "    scored_docs = list(zip(normalized_scores, docs))\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        for i, (score, doc) in enumerate(scored_docs[:top_n], start=1):\n",
    "            print(f\"\\nRank {i} Score: {score:.4f}\")\n",
    "            print(doc.page_content[:300] + \"...\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    return scored_docs[:top_n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3038e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e18c996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant(ground_truth: str, doc_content: str, threshold: float = 80) -> bool:\n",
    "    ground_truth = ground_truth.lower().strip()\n",
    "    doc_content = doc_content.lower().strip()\n",
    "\n",
    "    # Exact substring match\n",
    "    if ground_truth in doc_content:\n",
    "        return True\n",
    "\n",
    "    # Fuzzy matching for paraphrased content\n",
    "    similarity = fuzz.partial_ratio(ground_truth, doc_content)\n",
    "    return similarity >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d1d0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(\n",
    "    eval_data: List[Dict[str, Any]],\n",
    "    retriever: Any,\n",
    "    reranker: Any,\n",
    "    k: int = 3,\n",
    "    fuzzy_threshold: float = 70,\n",
    "    doc_content_key: str = 'page_content'\n",
    ") -> Dict[str, float]:\n",
    "    hits = 0  # Count of queries with at least one relevant document\n",
    "    relevant_docs_total = 0  # Total number of relevant documents retrieved\n",
    "    retrieved_docs_total = 0  # Total number of documents retrieved\n",
    "    reciprocal_ranks = []  # Reciprocal ranks for MRR\n",
    "\n",
    "    for sample in tqdm(eval_data, desc=\"Evaluating\"):\n",
    "        question = sample[\"question\"]\n",
    "        ground_truth = sample[\"answer\"]\n",
    "\n",
    "        try:\n",
    "            # Retrieve initial documents\n",
    "            initial_results = retriever.get_relevant_documents(question)\n",
    "            if not initial_results:\n",
    "                reciprocal_ranks.append(0)\n",
    "                continue\n",
    "\n",
    "            # Rerank\n",
    "            reranked = reranker(question, initial_results, top_n=k)\n",
    "            if not reranked:\n",
    "                reciprocal_ranks.append(0)\n",
    "                continue\n",
    "\n",
    "            # Track relevance\n",
    "            found = False\n",
    "            retrieved_docs_total += len(reranked)\n",
    "            for rank, (score, doc) in enumerate(reranked[:k]):\n",
    "                try:\n",
    "                    # Extract document content dynamically\n",
    "                    doc_content = getattr(doc, doc_content_key, doc) if isinstance(doc, object) else doc\n",
    "                    if isinstance(doc_content, dict):\n",
    "                        doc_content = doc_content.get('content', '')\n",
    "\n",
    "                    # Check relevance\n",
    "                    if is_relevant(ground_truth, doc_content, fuzzy_threshold):\n",
    "                        relevant_docs_total += 1\n",
    "                        if not found:\n",
    "                            hits += 1\n",
    "                            reciprocal_ranks.append(1 / (rank + 1))\n",
    "                            found = True\n",
    "\n",
    "                except AttributeError as e:\n",
    "                    print(f\"Warning: Could not access document content for rank {rank}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if not found:\n",
    "                reciprocal_ranks.append(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question '{question}': {e}\")\n",
    "            reciprocal_ranks.append(0)\n",
    "            continue\n",
    "\n",
    "    # Calculate metrics\n",
    "    total_queries = len(eval_data)\n",
    "    recall = hits / total_queries if total_queries > 0 else 0.0\n",
    "    mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "    return {\n",
    "        f\"Recall@{k}\": recall,\n",
    "        f\"MRR@{k}\": mrr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be369b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_data = load_eval_data(\"test.json\")\n",
    "\n",
    "results = evaluate_retriever(\n",
    "    eval_data=eval_data,\n",
    "    retriever=ensemble_retriever,\n",
    "    reranker=rerank_with_cross_encoder,\n",
    "    k=3,\n",
    "    fuzzy_threshold=80,\n",
    "    doc_content_key='page_content'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ce91eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Recall@3: 0.6667\n",
      "MRR@3: 0.5000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "427e5e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 | Score: -0.0278\n",
      "------------------------------------------------------------\n",
      "Tangi sa pampalagiang suliranin na pangkabuhayan at pampamahalaan ang lumigalig sa kanyang pangasiwaan. == Pamumuno ni Ferdinand Marcos (1965–1986) ==\n",
      "\n",
      "Ang isa pang subyang sa panig ni Macapagal ay ang Pangulo ng Senado na si Ferdinand Marcos, isang kapwa Liberal. Sinasabing upang matamo ang pagtulo\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 2 | Score: -0.0726\n",
      "------------------------------------------------------------\n",
      "Tacloban at suporta galing sa Tanggapan ng Bise Presidente para sa Usaping Pang-Akademiko ng Unibersidad ng Pilipinas (Inggles: University of the Philippines Office of the Vice President for Academic Affairs) sa tulong ng Pondo para sa Pagpapataas ng Makasining na Paggawa at Pagsasaliksik (Inggles:\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Output\n",
    "query = \"Who was the first President of the Philippines?\"\n",
    "\n",
    "# Retrieve documents using hybrid retriever\n",
    "initial_results = ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "# Rerank using Cross-Encoder\n",
    "reranked = rerank_with_cross_encoder(query, initial_results, top_n=2)\n",
    "\n",
    "for i, (score, doc) in enumerate(reranked, start=1):\n",
    "    print(f\"\\nRank {i} | Score: {score:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(doc.page_content.strip()[:300])\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
