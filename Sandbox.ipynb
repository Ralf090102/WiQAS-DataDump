{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebb57e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import wikipedia\n",
    "import torch\n",
    "import wikipediaapi\n",
    "import time\n",
    "import fitz\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1abd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"history-of-the-filipino-people.pdf\"\n",
    "json_path = \"knowledge_base/filipino_history_pdf.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a51619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping PDF extraction – file already exists: knowledge_base/filipino_history_pdf.json\n"
     ]
    }
   ],
   "source": [
    "# Scrapping History of the Filipino People PDF\n",
    "if os.path.exists(json_path):\n",
    "    print(f\"[INFO] Skipping PDF extraction – file already exists: {json_path}\")\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    print(f\"[INFO] Extracting text from: {pdf_path}\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    data = {}\n",
    "\n",
    "    # Extract text from each page\n",
    "    for page_number in tqdm(range(len(doc)), desc=\"Extracting PDF\"):\n",
    "        page = doc[page_number]\n",
    "        text = page.get_text().strip()\n",
    "\n",
    "        if text:\n",
    "            data[f\"page_{page_number + 1}\"] = {\n",
    "                \"page\": page_number + 1,\n",
    "                \"content\": text\n",
    "            }\n",
    "\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[INFO] Saved PDF content to {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5bacfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wikipedia API for English and Tagalog\n",
    "wiki_en = wikipediaapi.Wikipedia(language='en', user_agent='WiQAS/1.0 (ralf_hernandez@dlsu.edu.ph)')\n",
    "wiki_tl = wikipediaapi.Wikipedia(language='tl', user_agent='WiQAS/1.0 (ralf_hernandez@dlsu.edu.ph)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efb55d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all pages in a category recursively\n",
    "def scrape_category(category, wiki, scraped_pages=None, depth=0, max_depth=7):\n",
    "    if scraped_pages is None:\n",
    "        scraped_pages = {}\n",
    "    \n",
    "    # Avoid infinite recursion\n",
    "    if depth > max_depth:\n",
    "        return scraped_pages\n",
    "    \n",
    "    # Get category members\n",
    "    cat = wiki.page(f\"Category:{category}\")\n",
    "    if not cat.exists():\n",
    "        return scraped_pages\n",
    "    \n",
    "    # Iterate through category members\n",
    "    for member_name, member_page in tqdm(cat.categorymembers.items(), desc=f\"Scraping {category}\"):\n",
    "        if member_page.namespace == wikipediaapi.Namespace.CATEGORY:\n",
    "            # Recursively scrape subcategories\n",
    "            print(f\"[INFO] Processing: {member_name} | Depth: {depth}\")\n",
    "            scrape_category(member_name.replace(\"Category:\", \"\"), wiki, scraped_pages, depth + 1, max_depth)\n",
    "        else:\n",
    "            # Process articles\n",
    "            try:\n",
    "                if member_page.exists() and member_name not in scraped_pages:\n",
    "                    scraped_pages[member_name] = {\n",
    "                        'title': member_page.title,\n",
    "                        'content': member_page.text,\n",
    "                        'url': member_page.fullurl\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {member_name}: {e}\")\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    return scraped_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English Wikipedia scraping\n",
    "english_json_path = \"knowledge_base/philippine_culture_en.json\"\n",
    "\n",
    "if os.path.exists(english_json_path):\n",
    "    print(f\"[INFO] Skipping English scrape – file already exists: {english_json_path}\")\n",
    "    with open(english_json_path, 'r', encoding='utf-8') as f:\n",
    "        english_data = json.load(f)\n",
    "else:\n",
    "    print(\"\\n[INFO] Scraping English category: Culture_of_the_Philippines\")\n",
    "    english_data = scrape_category('Culture_of_the_Philippines', wiki_en)\n",
    "\n",
    "    with open(english_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(english_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[INFO] Saved to {english_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f630c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping Tagalog scrape – file already exists: knowledge_base/philippine_culture_tl.json\n"
     ]
    }
   ],
   "source": [
    "# Tagalog Wikipedia scraping\n",
    "tagalog_json_path = \"knowledge_base/philippine_culture_tl.json\"\n",
    "\n",
    "if os.path.exists(tagalog_json_path):\n",
    "    print(f\"[INFO] Skipping Tagalog scrape – file already exists: {tagalog_json_path}\")\n",
    "    with open(tagalog_json_path, 'r', encoding='utf-8') as f:\n",
    "        tagalog_data = json.load(f)\n",
    "else:\n",
    "    print(\"\\n[INFO] Scraping Tagalog category: Kultura_ng_Pilipinas\")\n",
    "    tagalog_data = scrape_category('Kultura_ng_Pilipinas', wiki_tl)\n",
    "\n",
    "    with open(tagalog_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tagalog_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[INFO] Saved to {tagalog_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56611181",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_dir = \"knowledge_base\"\n",
    "processed_dir = \"knowledge_base/cleaned\"\n",
    "os.makedirs(processed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "959709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def clean_text(text):\n",
    "    # Basic cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = re.sub(r'\\.{3,}', '.', text)  # Collapse ellipsis\n",
    "    text = re.sub(r'\\s+\\.', '.', text)  # Remove space before periods\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove common noise headers (language-dependent)\n",
    "    for noisy_section in [\"See also\", \"References\", \"Mga sanggunian\", \"Tingnan din\"]:\n",
    "        if noisy_section.lower() in text.lower():\n",
    "            text = text[:text.lower().find(noisy_section.lower())]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing filipino_history_pdf.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning filipino_history_pdf.json: 100%|██████████| 645/645 [00:00<00:00, 4182.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleaned file saved to: knowledge_base/cleaned\\filipino_history_pdf.json\n",
      "[INFO] Processing philippine_culture_tl.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning philippine_culture_tl.json: 100%|██████████| 93/93 [00:00<00:00, 2364.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleaned file saved to: knowledge_base/cleaned\\philippine_culture_tl.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all JSON files\n",
    "for filename in os.listdir(kb_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        input_path = os.path.join(kb_dir, filename)\n",
    "        output_path = os.path.join(processed_dir, filename)\n",
    "\n",
    "        print(f\"[INFO] Processing {filename}\")\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        cleaned_data = {}\n",
    "\n",
    "        for key, value in tqdm(raw_data.items(), desc=f\"Cleaning {filename}\"):\n",
    "            text = value.get(\"content\") or value.get(\"text\") or \"\"\n",
    "            cleaned = clean_text(text)\n",
    "            if len(cleaned.split()) >= 5:\n",
    "                cleaned_data[key] = {\n",
    "                    **value,\n",
    "                    \"cleaned_content\": cleaned\n",
    "                }\n",
    "\n",
    "        # Save cleaned output\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Cleaned file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3f96891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ralfh\\AppData\\Local\\Temp\\ipykernel_5984\\826302932.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Dense Embeddings (BGE-M3)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e892a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74312637",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for doc in cleaned_data.values():\n",
    "    text = doc[\"cleaned_content\"]\n",
    "    split_chunks = splitter.split_text(text)\n",
    "    chunks.extend(split_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f62001b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma Vector Store (Dense Retrieval)\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"filipino_culture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23fd0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks)\n",
    "bm25_retriever.k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ea1b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        bm25_retriever\n",
    "    ],\n",
    "    weights=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1dc38cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, docs, top_n=2):\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    scored_docs = list(zip(scores, docs))\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "    return scored_docs[:top_n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 | Score: 5.7913\n",
      "------------------------------------------------------------\n",
      ". Naging Direktor din siya ng Bayanihan Philippine Dance Company. Mga nagawa Itinatag ni Lucrecia Reyes-Urtula ang The Bayanihan Dance Company of the Philippines noong 1957. Dito ay gumawa siya ng mga koreograpo sa mahigit na tatlumpong taon ng mga sayaw tulad ng mga sayaw sa bulubundukin, mga sayaw na may impluwensiya ng mga Kastila, mga sayaw na tungkol sa mga pagdiriwang ng mga Muslim, at mga sayaw sa kabukiran\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 2 | Score: 5.4748\n",
      "------------------------------------------------------------\n",
      ". Mga nagawa Si Lucrecia Kasilag ang dahilan sa pagkakatatag ng Bayanihan Folk Arts Center noong 1957 at katuwang na tagapagtatag ng Bayanihan National Dance Company. Bilang kompositor, nakalikha si Lucrecia Kasilag ng 250 komposiyon, mga areglo ng mga katutubong awit, awit sining, mga piyesang pang-solo at instrumental at mga akdang pang-orkestra\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 3 | Score: 4.5032\n",
      "------------------------------------------------------------\n",
      ". Kadalasan makikita ang bayanihan sa mga sasakyang nasisiraan ng gulong. Ang mga tambay at ang mga taong-bayang na malapit dito ay agad agad ding tutulungan ang drayber kahit ano pa man ang mangyari maayos lamang ang nasirang sasakyan. O kaya naman mas kadalasang inilalarawan ito ng paglilipat bahay noon ng mga nasa lalawigan. Ang mga bahay ay sabay sabay bubuhatin ng mga kalalakihan na sinasabayan pa kung minsan ng awitin upang di gaanong madama ang kabigatan nito\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 4 | Score: 4.0903\n",
      "------------------------------------------------------------\n",
      ". Ang bansa ay nahahati sa pagitan ng mga Kristiyano, Muslim, at iba pang pangkat; sa pagitan ng mga nasa lungsod at sa mga nayon; mga tagabundok at tagapatag; at pagitan ng mga mayayaman at ng mga mahihirap. Kaugaliang Pilipino Bayanihan: Nabuo ang Bayanihan sa mga samahan ng mga magkakapitbahay na nagtutulungan kahit kailan o saan man kailanganin ng tulong. Kadalasan makikita ang bayanihan sa mga sasakyang nasisiraan ng gulong\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 5 | Score: -10.9529\n",
      "------------------------------------------------------------\n",
      "p.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Output\n",
    "query = \"ang diwa ng pagtutulungan at suportang ibinibigay ng mga miyembro ng komunidad sa isa't isa\"\n",
    "\n",
    "# Retrieve documents using hybrid retriever\n",
    "initial_results = ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "# Rerank using Cross-Encoder\n",
    "reranked = rerank_with_cross_encoder(query, initial_results, top_n=5)\n",
    "\n",
    "for i, (score, doc) in enumerate(reranked, start=1):\n",
    "    print(f\"\\nRank {i} | Score: {score:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(doc.page_content.strip()[:500])\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
