{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebb57e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import wikipedia\n",
    "import torch\n",
    "import wikipediaapi\n",
    "import time\n",
    "import fitz\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d174d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_section_title(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if re.match(r'^(Chapter \\d+|[\\dI]+\\.\\s+[A-Za-z\\s]+)', line, re.IGNORECASE):\n",
    "            return line\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1abd989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created directory: knowledge_base\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"history-of-the-filipino-people.pdf\"\n",
    "json_path = \"knowledge_base/filipino_history_pdf.json\"\n",
    "directory = os.path.dirname(json_path)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"[INFO] Created directory: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a51619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting text from: history-of-the-filipino-people.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting PDF: 100%|██████████| 662/662 [00:00<00:00, 722.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved PDF content to knowledge_base/filipino_history_pdf.json\n"
     ]
    }
   ],
   "source": [
    "# Scrapping History of the Filipino People PDF\n",
    "if os.path.exists(json_path):\n",
    "    print(f\"[INFO] Skipping PDF extraction – file already exists: {json_path}\")\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    print(f\"[INFO] Extracting text from: {pdf_path}\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    data = {}\n",
    "\n",
    "    # Extract text from each page\n",
    "    for page_number in tqdm(range(len(doc)), desc=\"Extracting PDF\"):\n",
    "        page = doc[page_number]\n",
    "        text = page.get_text().strip()\n",
    "\n",
    "        if text:\n",
    "            # Detect section title (optional)\n",
    "            section = detect_section_title(text)\n",
    "            \n",
    "            data[f\"page_{page_number + 1}\"] = {\n",
    "                \"page\": page_number + 1,\n",
    "                \"content\": text,\n",
    "                \"metadata\": {\n",
    "                    \"source\": \"Agoncillo_textbook\",\n",
    "                    \"document_type\": \"textbook\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"section\": section\n",
    "                }\n",
    "            }\n",
    "\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[INFO] Saved PDF content to {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5bacfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wikipedia API for English and Tagalog\n",
    "wiki_en = wikipediaapi.Wikipedia(language='en', user_agent='WiQAS/1.0 (ralf_hernandez@dlsu.edu.ph)')\n",
    "wiki_tl = wikipediaapi.Wikipedia(language='tl', user_agent='WiQAS/1.0 (ralf_hernandez@dlsu.edu.ph)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efb55d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all pages in a category recursively\n",
    "def scrape_category(category, wiki, language, scraped_pages=None, depth=0, max_depth=3):\n",
    "    if scraped_pages is None:\n",
    "        scraped_pages = {}\n",
    "    \n",
    "    # Avoid infinite recursion\n",
    "    if depth > max_depth:\n",
    "        return scraped_pages\n",
    "    \n",
    "    # Get category members\n",
    "    cat = wiki.page(f\"Category:{category}\")\n",
    "    if not cat.exists():\n",
    "        return scraped_pages\n",
    "    \n",
    "    # Iterate through category members\n",
    "    for member_name, member_page in tqdm(cat.categorymembers.items(), desc=f\"Scraping {category}\"):\n",
    "        if member_page.namespace == wikipediaapi.Namespace.CATEGORY:\n",
    "            print(f\"[INFO] Processing: {member_name} | Depth: {depth}\")\n",
    "            scrape_category(member_name.replace(\"Category:\", \"\"), wiki, language, scraped_pages, depth + 1, max_depth)\n",
    "        else:\n",
    "            try:\n",
    "                if member_page.exists() and member_name not in scraped_pages:\n",
    "                    scraped_pages[member_name] = {\n",
    "                        \"title\": member_page.title,\n",
    "                        \"content\": member_page.text,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": f\"Wikipedia_{language}\",\n",
    "                            \"language\": \"English\" if language == \"en\" else \"Tagalog\",\n",
    "                            \"url\": member_page.fullurl,\n",
    "                            \"category\": category,\n",
    "                            \"namespace\": \"Main\"\n",
    "                        }\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {member_name}: {e}\")\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    return scraped_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa5915",
   "metadata": {},
   "source": [
    "# TAKES TOO LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd2d8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English Wikipedia scraping\n",
    "# english_json_path = \"knowledge_base/philippine_culture_en.json\"\n",
    "\n",
    "# if os.path.exists(english_json_path):\n",
    "#     print(f\"[INFO] Skipping English scrape – file already exists: {english_json_path}\")\n",
    "#     with open(english_json_path, 'r', encoding='utf-8') as f:\n",
    "#         english_data = json.load(f)\n",
    "# else:\n",
    "#     print(\"\\n[INFO] Scraping English category: Culture_of_the_Philippines\")\n",
    "#     english_data = scrape_category('Culture_of_the_Philippines', wiki_en, language=\"en\")\n",
    "\n",
    "#     with open(english_json_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(english_data, f, ensure_ascii=False, indent=2)\n",
    "#     print(f\"[INFO] Saved to {english_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f630c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Scraping Tagalog category: Kultura_ng_Pilipinas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  82%|████████▏ | 93/114 [01:06<00:14,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Gawad Ramon Magsaysay | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  82%|████████▏ | 94/114 [01:06<00:11,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Kultura ng Pilipinas ayon sa lalawigan | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  83%|████████▎ | 95/114 [01:07<00:10,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Kulturang Bisaya | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  84%|████████▍ | 96/114 [01:07<00:08,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Lutuing Pilipino | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  85%|████████▌ | 97/114 [01:08<00:07,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga awiting Pilipino | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  86%|████████▌ | 98/114 [01:08<00:07,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga manlililok mula sa Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  87%|████████▋ | 99/114 [01:08<00:06,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga pambansang alagad ng sining ng Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  88%|████████▊ | 100/114 [01:09<00:05,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga pambansang sagisag ng Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  89%|████████▊ | 101/114 [01:09<00:05,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga pangkat-etniko sa Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  89%|████████▉ | 102/114 [01:10<00:05,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga patimpalak sa Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  90%|█████████ | 103/114 [01:10<00:04,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga sining panlaban ng Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  91%|█████████ | 104/114 [01:10<00:04,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga tinapay ng Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  92%|█████████▏| 105/114 [01:11<00:03,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mitolohiyang Pilipino | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  93%|█████████▎| 106/114 [01:11<00:03,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Musika ng Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  94%|█████████▍| 107/114 [01:12<00:02,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Palakasan sa Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  95%|█████████▍| 108/114 [01:12<00:02,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Pambansang Alagad ng Sining ng Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  96%|█████████▌| 109/114 [01:13<00:02,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga pelikula mula sa Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  96%|█████████▋| 110/114 [01:13<00:01,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Relihiyon sa Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  97%|█████████▋| 111/114 [01:13<00:01,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Sayaw sa Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  98%|█████████▊| 112/114 [01:14<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Sining ng Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas:  99%|█████████▉| 113/114 [01:14<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: Kategorya:Mga wika ng Pilipinas | Depth: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Kultura_ng_Pilipinas: 100%|██████████| 114/114 [01:15<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved to knowledge_base/philippine_culture_tl.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tagalog Wikipedia scraping\n",
    "tagalog_json_path = \"knowledge_base/philippine_culture_tl.json\"\n",
    "\n",
    "if os.path.exists(tagalog_json_path):\n",
    "    print(f\"[INFO] Skipping Tagalog scrape – file already exists: {tagalog_json_path}\")\n",
    "    with open(tagalog_json_path, 'r', encoding='utf-8') as f:\n",
    "        tagalog_data = json.load(f)\n",
    "else:\n",
    "    print(\"\\n[INFO] Scraping Tagalog category: Kultura_ng_Pilipinas\")\n",
    "    tagalog_data = scrape_category('Kultura_ng_Pilipinas', wiki_tl, language=\"tl\")\n",
    "\n",
    "    with open(tagalog_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tagalog_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[INFO] Saved to {tagalog_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56611181",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_dir = \"./knowledge_base\"\n",
    "processed_dir = \"./processed_data\"\n",
    "os.makedirs(processed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "959709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def clean_text(text):\n",
    "    # Normalize whitespace and punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\.{3,}', '.', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    # Remove Wikipedia-specific noise\n",
    "    text = re.sub(r'\\{\\{.*?\\}\\}', '', text)  # Remove templates\n",
    "    text = re.sub(r'\\[\\[.*?\\]\\]', '', text)  # Remove internal links\n",
    "    text = re.sub(r'\\[edit\\]', '', text)  # Remove edit tags\n",
    "    text = re.sub(r'Category:.*|Kategorya:.*', '', text)  # Remove category tags\n",
    "    \n",
    "    # Remove common Wikipedia and textbook sections\n",
    "    noisy_sections = [\n",
    "        \"See also\", \"References\", \"External links\", \"Mga sanggunian\", \n",
    "        \"Tingnan din\", \"Mga kawing panlabas\", \"Table of Contents\", \n",
    "        \"Preface\", \"Foreword\", \"Appendix\", \"Talaan ng mga nilalaman\"\n",
    "    ]\n",
    "    for section in noisy_sections:\n",
    "        if section.lower() in text.lower():\n",
    "            text = text[:text.lower().find(section.lower())]\n",
    "    \n",
    "    # Remove short or low-value text\n",
    "    text = text.strip()\n",
    "    return text if len(text.split()) >= 5 else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing filipino_history_pdf.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning filipino_history_pdf.json: 100%|██████████| 645/645 [00:00<00:00, 3140.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleaned file saved to: ./processed_data\\filipino_history_pdf.json\n",
      "[INFO] Processing philippine_culture_tl.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning philippine_culture_tl.json: 100%|██████████| 93/93 [00:00<00:00, 1705.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleaned file saved to: ./processed_data\\philippine_culture_tl.json\n"
     ]
    }
   ],
   "source": [
    "# Process all JSON files\n",
    "for filename in os.listdir(kb_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        input_path = os.path.join(kb_dir, filename)\n",
    "        output_path = os.path.join(processed_dir, filename)\n",
    "\n",
    "        print(f\"[INFO] Processing {filename}\")\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        cleaned_data = {}\n",
    "\n",
    "        for key, value in tqdm(raw_data.items(), desc=f\"Cleaning {filename}\"):\n",
    "            text = value.get(\"content\") or value.get(\"text\") or \"\"\n",
    "            cleaned = clean_text(text)\n",
    "            if len(cleaned.split()) >= 5:\n",
    "                metadata = value.get(\"metadata\", {})\n",
    "                if \"source\" not in metadata:\n",
    "                    if \"Agoncillo\" in filename:\n",
    "                        metadata[\"source\"] = \"Agoncillo_textbook\"\n",
    "                    elif \"en\" in filename:\n",
    "                        metadata[\"source\"] = \"Wikipedia_en\"\n",
    "                    elif \"tl\" in filename:\n",
    "                        metadata[\"source\"] = \"Wikipedia_tl\"\n",
    "                    else:\n",
    "                        metadata[\"source\"] = \"unknown\"\n",
    "                metadata[\"source_file\"] = filename\n",
    "                cleaned_data[key] = {\n",
    "                    **value,\n",
    "                    \"cleaned_content\": cleaned,\n",
    "                    \"metadata\": metadata\n",
    "                }\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"[INFO] Cleaned file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f96891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Embeddings (BGE-M3)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate Chunks\n",
    "def deduplicate_chunks(chunks_with_metadata, threshold=0.95):\n",
    "    texts = [c[\"text\"] for c in chunks_with_metadata]\n",
    "    vectorizer = TfidfVectorizer().fit_transform(texts)\n",
    "    similarities = cosine_similarity(vectorizer)\n",
    "    unique_chunks = []\n",
    "    seen = set()\n",
    "    for i, chunk in enumerate(chunks_with_metadata):\n",
    "        if i not in seen:\n",
    "            unique_chunks.append(chunk)\n",
    "            for j in range(i + 1, len(texts)):\n",
    "                if similarities[i, j] > threshold:\n",
    "                    seen.add(j)\n",
    "    return unique_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=75,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74312637",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     text = doc[\u001b[33m\"\u001b[39m\u001b[33mcleaned_content\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m     split_chunks = splitter.split_text(text)\n\u001b[32m      5\u001b[39m     chunks_with_metadata.extend([\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: chunk, \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msource\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33msection\u001b[39m\u001b[33m\"\u001b[39m: doc.get(\u001b[33m\"\u001b[39m\u001b[33msection\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)}}\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m split_chunks\n\u001b[32m      8\u001b[39m     ])\n\u001b[32m      9\u001b[39m chunks = deduplicate_chunks([c[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks_with_metadata])\n\u001b[32m     10\u001b[39m metadata = [c[\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks_with_metadata][:\u001b[38;5;28mlen\u001b[39m(chunks)]\n",
      "\u001b[31mKeyError\u001b[39m: 'source'"
     ]
    }
   ],
   "source": [
    "chunks_with_metadata = []\n",
    "for key, doc in cleaned_data.items():\n",
    "    text = doc[\"cleaned_content\"]\n",
    "    # Access metadata safely\n",
    "    metadata = doc.get(\"metadata\", {})\n",
    "    source = metadata.get(\"source\", \"unknown\")\n",
    "    section = metadata.get(\"section\", \"unknown\")\n",
    "    split_chunks = splitter.split_text(text)\n",
    "    chunks_with_metadata.extend([\n",
    "        {\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"source\": source,\n",
    "                \"section\": section,\n",
    "                \"source_file\": metadata.get(\"source_file\", \"unknown\"),\n",
    "                \"chunk_id\": f\"{key}_{i}\"\n",
    "            }\n",
    "        }\n",
    "        for i, chunk in enumerate(split_chunks)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d12713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate\n",
    "chunks_with_metadata = deduplicate_chunks(chunks_with_metadata)\n",
    "chunks = [c[\"text\"] for c in chunks_with_metadata]\n",
    "metadata = [c[\"metadata\"] for c in chunks_with_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62001b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma Vector Store (Dense Retrieval)\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    metadatas=metadata,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"filipino_culture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks)\n",
    "bm25_retriever.k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        bm25_retriever\n",
    "    ],\n",
    "    weights=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc38cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def rerank_with_cross_encoder(query, docs, top_n=3, verbose=False):\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    normalized_scores = [score / (len(doc.page_content.split()) + 1) for score, doc in zip(scores, docs)]\n",
    "    scored_docs = list(zip(normalized_scores, docs))\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        for i, (score, doc) in enumerate(scored_docs[:top_n], start=1):\n",
    "            print(f\"\\nRank {i} Score: {score:.4f}\")\n",
    "            print(doc.page_content[:300] + \"...\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    return scored_docs[:top_n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 Score: 6.5149\n",
      ". Ang ibig sabihin ng kultura ay ang paraan ng pamumuhay ng mga tao nagpapakita ng kaugalian, tradisyon, mga sining, sistema ng edukasyon, musika at pamahalaan. Lipunang Pilipino Ang Lipunang Pilipino ay magkahalong lipunan. Isa bilang bansa, at marami dahil sa pagkakahiwalay ng mga ito ng lugar, dahil sa pulo pulo nitong ayos at mga kasanayan...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 2 Score: 5.1147\n",
      ". • Ang Pagaandukha na konsepto ng Sikolohiyang Pilipino ay ang pagkukuha ng salitang dayuhan at baguhin ang kanyang anyo hangga’t magkaroon siya ng Pilipinong kahulugan. • Ang konsepto ng Pagbibinyag sa Sikolohiyang Pilipino ay madali lang intindihan sapagkat ang ibig sabihin nito ay ang paglalagay ng mga dayuhan ng kanilang mga sariling kahulugan...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Rank 3 Score: 3.5327\n",
      "Isa bilang bansa, at marami dahil sa pagkakahiwalay ng mga ito ng lugar, dahil sa pulo pulo nitong ayos at mga kasanayan. Ang bansa ay nahahati sa pagitan ng mga Kristiyano, Muslim, at iba pang pangkat; sa pagitan ng mga nasa lungsod at sa mga nayon; mga tagabundok at tagapatag; at pagitan ng mga mayayaman at ng mga mahihirap. === Kaugaliang Pilipi...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Output\n",
    "query = \"Ano ang ibig sabihin ng “bayanihan” sa kulturang Pilipino?\"\n",
    "# Retrieve documents using hybrid retriever\n",
    "initial_results = ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "# Rerank using Cross-Encoder\n",
    "reranked = rerank_with_cross_encoder(query, initial_results, top_n=3, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
